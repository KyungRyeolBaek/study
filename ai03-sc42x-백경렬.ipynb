{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ai_sc42x.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "u4-s1-nlp"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "nteract": {
      "version": "0.22.4"
    },
    "toc-autonumbering": false
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyungRyeolBaek/study/blob/main/ai03-sc42x-%EB%B0%B1%EA%B2%BD%EB%A0%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy_9hARVUo9L"
      },
      "source": [
        "# SC42x \n",
        "## 자연어처리 (Natural Language Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dNUXHiLbBdp"
      },
      "source": [
        "# Part 1 : 개념 요약\n",
        "\n",
        "> 다음의 키워드에 대해서 **한 줄**로 간단하게 요약해주세요. (세션 노트를 참고하여도 좋습니다.)<br/>\n",
        "> **Tip : 아래 문제를 먼저 수행한 후 모델 학습 등 시간이 오래 걸리는 셀이 실행되는 동안 아래 내용을 작성하면 시간을 절약할 수 있습니다.**\n",
        "\n",
        "**N421**\n",
        "- Stopwords(불용어) : 분석할 때 의미가 없는 단어 예를 들어 제품 리뷰를 이해 하는데 I, and, of 같은 단어들은 의미가 없는데 이들을 불용어라고 한다.\n",
        "- Stemming과 Lemmatization : 어간 추출(Stemming)이란 단어의 의미가 포함된 부분으로 접사등이 제거된 형태로 만드는 것으로 예를 들어 argue, argued, arguing, argus의 뒷부분인 ing, ed, s를 제거 하는 과정이며 argu라는 어간을 뽑아내는 과정이며,\n",
        "표제어 추출(Lemmatization)이란 어간 추출보다 체계적으로 단어들의 기본 사전형 단어 형태인 표제어로 변환하는 과정이며 예를 들어 명사의 복수형은 단수형으로, 동사는 모두 타동사로 변환하는 과정을 말한다.\n",
        "- Bag-of-Words : 가장 단순한 벡터화 방법 중 하나로 문서 혹은 문장에서 문법이나 단어의 순서 등을 무시하고 단순히 단어들의 빈도만 고려하여 벡터화 하는 과정\n",
        "- TF-IDF : 전체적인 문서군에서 자주 등장하는 단어들은 DF ( 문서 빈도 ) 라고 하며, 이 값의 역수를 IDF ( 역문서 빈도 ) 라 하고, 그리고 그 값과 TF ( 단어 빈도 ) 값을 곱해 준 값이 TF-IDF 라고 한다.\n",
        "\n",
        "**N422**\n",
        "- Word2Vec : 단어를 벡터로 나타내는 가장 널리 쓰이는 임베딩 방법 중 하나.\n",
        "- fastText : Word2Vec 방식에 철자 (Character) 기반의 임베딩 방식을 더한 새로운 임베딩 방식이다.\n",
        "\n",
        "**N423**\n",
        "- RNN : 연속형 데이터들을 잘 처리하기 위해 고안된 신경망으로 기존 신경망에는 없던 출력 벡터가 다시 입력되는 특성 때문에 순환(Recurrent) 신경망 이라는 이름이 붙었다.\n",
        "- LSTM, GRU : LSTM은 Long-Short Term Memory로 장단기 기억망이라 하며 RNN의 기울기 정보 크기를 조절하고자 Gate를 3개 추가했으며, 내부의 Cell-state는 역전파 과정에서 활성화 함수를 거치지 않아 앞쪽 시퀀스의 정보를 어느정도 잃지 않게 해주는 모델이며, \n",
        "GRU는 Gate Recurrent Unit으로 LSTM의 간소화 버전이며, LSTM의 Output Gate가 없어졌으며, Cell-state의 벡터와 hidden-state 벡터가 통합되어 하나의 벡터로 출력된다.\n",
        "- Attention : 인코더에서 각 Time-step 마다 생성되는 hidden-state 벡터들을 모아서 디코더의 hidden-state 벡터와 내적한 값을 이용해 모든 단어가 영향력을 잃지 않게 해주는 과정이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g07_yjCgbBdp"
      },
      "source": [
        "# Part 2 : Fake/Real News Dataset\n",
        "\n",
        "한 주간 자연어처리 기법을 배우면서 여러분은 다양한 기술들을 접했습니다.<br/>\n",
        "어떻게 텍스트 데이터를 다뤄야 하는지, 텍스트를 벡터화 하는 법, 문서에서 토픽을 모델하는 법 등 다양한 NLP 기법을 배웠는데요.<br/>\n",
        "이번 스프린트 챌린지에선 [Fake/Real News Dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)을 사용하여 배운 것들을 복습해보는 시간을 갖겠습니다.\n",
        "\n",
        "**주의 : 모델의 성능을 최대한 끌어올리는 것이 아닌 모델 구동에 초점을 맞춰주세요.<br/>\n",
        "모든 문제를 완료한 후에도 \"시간이 남았다면\" 정확도를 올리는 것에 도전하시는 것을 추천드립니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCX5K0nP0ZKQ"
      },
      "source": [
        "# 코드 실행 전 seed를 지정하겠습니다.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C08-JiyNbdQy"
      },
      "source": [
        "## 2.0 데이터셋을 불러옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyndWa5hxEmk"
      },
      "source": [
        "- 위 캐글 링크에서 데이터셋을 받아 업로드 합니다.<br/>\n",
        "(직접 업로드하게 되면 시간이 꽤 걸리므로 **drive_mount** 나 **kaggle 연동**하시는 것을 추천드립니다.)\n",
        "\n",
        "- 'label' 열을 만들어 Fake = 1, True = 0 로 레이블링해줍니다.\n",
        "- 두 파일을 합쳐 하나의 데이터프레임에 저장해 준 후 데이터를 섞어줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uqDCdi5blfk",
        "outputId": "e85ce869-cd99-4370-9102-9d3d024eda3e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSUccDw1bf3r",
        "outputId": "d5038e25-3978-4c63-8638-2410902b828b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "fake = pd.read_csv('/content/drive/MyDrive/Fake.csv')\n",
        "true = pd.read_csv('/content/drive/MyDrive/True.csv')\n",
        "\n",
        "true['label'] = [0 for _ in range(true.shape[0])]\n",
        "fake['label'] = [0 for _ in range(fake.shape[0])]\n",
        "\n",
        "df = pd.concat([true, fake])\n",
        "\n",
        "true.shape, fake.shape, df.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21417, 5), (23481, 5), (44898, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbxEHBanUo9d"
      },
      "source": [
        "## 2.1 TF-IDF 를 활용하여 특정 뉴스와 유사한 뉴스 검색하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4R5mLMS5c2I"
      },
      "source": [
        "시간상 특별한 **전처리 없이** 아래 태스크를 수행하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "5i4RSybdgO62",
        "outputId": "3140f70f-a6c7-4e59-cdbc-591f8a33c043"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... label\n",
              "0  As U.S. budget fight looms, Republicans flip t...  ...     0\n",
              "1  U.S. military to accept transgender recruits o...  ...     0\n",
              "2  Senior U.S. Republican senator: 'Let Mr. Muell...  ...     0\n",
              "3  FBI Russia probe helped by Australian diplomat...  ...     0\n",
              "4  Trump wants Postal Service to charge 'much mor...  ...     0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2B-6Wkk5YGD"
      },
      "source": [
        "### 2.1.1 TFidfVectorizer를 사용하여 문서-단어 행렬(Document-Term Matrix) 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kw9OQwmUo9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "31cf6c91-1cf5-43a4-8c02-5af6111ee6e1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# TF-IDF vectorizer. 테이블을 작게 만들기 위해 max_features=15로 제한하였습니다.\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=15)\n",
        "\n",
        "# Fit 후 dtm을 만듭니다.(문서, 단어마다 tf-idf 값을 계산합니다)\n",
        "dtm_tfidf = tfidf.fit_transform(df.title)\n",
        "\n",
        "dtm_tfidf = pd.DataFrame(dtm_tfidf.todense(), columns=tfidf.get_feature_names())\n",
        "dtm_tfidf"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clinton</th>\n",
              "      <th>hillary</th>\n",
              "      <th>house</th>\n",
              "      <th>just</th>\n",
              "      <th>new</th>\n",
              "      <th>obama</th>\n",
              "      <th>president</th>\n",
              "      <th>republican</th>\n",
              "      <th>russia</th>\n",
              "      <th>says</th>\n",
              "      <th>state</th>\n",
              "      <th>trump</th>\n",
              "      <th>video</th>\n",
              "      <th>watch</th>\n",
              "      <th>white</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44893</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44894</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44895</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44896</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44897</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44898 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       clinton  hillary  house  just  new  ...  state  trump  video  watch  white\n",
              "0          0.0      0.0    0.0   0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "1          0.0      0.0    0.0   0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "2          0.0      0.0    0.0   0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "3          0.0      0.0    0.0   0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "4          0.0      0.0    0.0   0.0  0.0  ...    0.0    1.0    0.0    0.0    0.0\n",
              "...        ...      ...    ...   ...  ...  ...    ...    ...    ...    ...    ...\n",
              "44893      0.0      0.0    0.0   0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "44894      0.0      0.0    0.0   0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "44895      0.0      0.0    0.0   0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "44896      0.0      0.0    0.0   0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "44897      0.0      0.0    0.0   0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "\n",
              "[44898 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeTMNUMM5WgQ"
      },
      "source": [
        "### 2.1.2 KNN 알고리즘을 사용하여 유사한 문서 검색하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KjQvaVCUBEH"
      },
      "source": [
        "- **42번 인덱스의 문서**와 가장 유사한 **5개 문서(42번 포함)의 인덱스**와 **해당 인덱스의 레이블**을 나타내주세요.\n",
        "- NN 모델의 파라미터 중 `algorithm = 'kd_tree'` 로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBmsCLMlwP1p"
      },
      "source": [
        "# SpaCy 를 이용한 Tokenizing\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "def tokenize(document):\n",
        "    doc = nlp(document)\n",
        "    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True) and (token.is_alpha == True)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jjaQO8O6UKd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "9e0843d8-6687-424f-f002-d3a3f0da6252"
      },
      "source": [
        "    \"\"\"\n",
        "    args:\n",
        "        ngram_range = (min_n, max_n), min_n 개~ max_n 개를 갖는 n-gram(n개의 연속적인 토큰)을 토큰으로 사용합니다.\n",
        "        min_df = n : int, 최소 n개의 문서에 나타나는 토큰만 사용합니다.\n",
        "        max_df = m : float(0~1), m * 100% 이상 문서에 나타나는 토큰은 제거합니다.\n",
        "    \"\"\"\n",
        "\n",
        "tfidf_tuned = TfidfVectorizer(stop_words='english'\n",
        "                        ,tokenizer=tokenize\n",
        "                        ,ngram_range=(1,2)\n",
        "                        ,max_df=.7\n",
        "                        ,min_df=3\n",
        "                       )\n",
        "\n",
        "dtm_tfidf_tuned = tfidf_tuned.fit_transform(df['text'].iloc[0:10])\n",
        "dtm_tfidf_tuned = pd.DataFrame(dtm_tfidf_tuned.todense(), columns=tfidf_tuned.get_feature_names())\n",
        "dtm_tfidf_tuned.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accord</th>\n",
              "      <th>accuse</th>\n",
              "      <th>action</th>\n",
              "      <th>administration</th>\n",
              "      <th>agreement</th>\n",
              "      <th>ally</th>\n",
              "      <th>american</th>\n",
              "      <th>april</th>\n",
              "      <th>base</th>\n",
              "      <th>begin</th>\n",
              "      <th>bias</th>\n",
              "      <th>bias republican</th>\n",
              "      <th>billion</th>\n",
              "      <th>california</th>\n",
              "      <th>campaign</th>\n",
              "      <th>charge</th>\n",
              "      <th>charge trump</th>\n",
              "      <th>child</th>\n",
              "      <th>clinton</th>\n",
              "      <th>collusion</th>\n",
              "      <th>come</th>\n",
              "      <th>comment</th>\n",
              "      <th>congress</th>\n",
              "      <th>conservative</th>\n",
              "      <th>continue</th>\n",
              "      <th>counsel</th>\n",
              "      <th>counsel robert</th>\n",
              "      <th>country</th>\n",
              "      <th>defense</th>\n",
              "      <th>democratic</th>\n",
              "      <th>democrats</th>\n",
              "      <th>deny</th>\n",
              "      <th>department</th>\n",
              "      <th>despite</th>\n",
              "      <th>dollar</th>\n",
              "      <th>early</th>\n",
              "      <th>edit</th>\n",
              "      <th>election</th>\n",
              "      <th>end</th>\n",
              "      <th>expect</th>\n",
              "      <th>...</th>\n",
              "      <th>secretary</th>\n",
              "      <th>senate</th>\n",
              "      <th>senator</th>\n",
              "      <th>september</th>\n",
              "      <th>set</th>\n",
              "      <th>small</th>\n",
              "      <th>speak</th>\n",
              "      <th>special</th>\n",
              "      <th>special counsel</th>\n",
              "      <th>spend</th>\n",
              "      <th>spending</th>\n",
              "      <th>statement</th>\n",
              "      <th>states</th>\n",
              "      <th>supporter</th>\n",
              "      <th>talk</th>\n",
              "      <th>tax</th>\n",
              "      <th>team</th>\n",
              "      <th>tell</th>\n",
              "      <th>think</th>\n",
              "      <th>time</th>\n",
              "      <th>times</th>\n",
              "      <th>trump administration</th>\n",
              "      <th>trump ally</th>\n",
              "      <th>trump say</th>\n",
              "      <th>twitt</th>\n",
              "      <th>twitter</th>\n",
              "      <th>united</th>\n",
              "      <th>united states</th>\n",
              "      <th>use</th>\n",
              "      <th>want</th>\n",
              "      <th>washington</th>\n",
              "      <th>washington reuters</th>\n",
              "      <th>way</th>\n",
              "      <th>wednesday</th>\n",
              "      <th>week</th>\n",
              "      <th>white</th>\n",
              "      <th>white house</th>\n",
              "      <th>work</th>\n",
              "      <th>york</th>\n",
              "      <th>york times</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.083416</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083416</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093824</td>\n",
              "      <td>0.140736</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.140736</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>0.23456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.125125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.23456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.375296</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.187648</td>\n",
              "      <td>0.067723</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.093824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.572920</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.171876</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.114584</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128881</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046513</td>\n",
              "      <td>0.257761</td>\n",
              "      <td>0.051451</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.257761</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051451</td>\n",
              "      <td>...</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128881</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.139540</td>\n",
              "      <td>0.046513</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.114584</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.210888</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.316332</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.152220</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042095</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.046874</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.042095</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.158166</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046874</td>\n",
              "      <td>0.158166</td>\n",
              "      <td>0.076110</td>\n",
              "      <td>0.038055</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.038055</td>\n",
              "      <td>0.038055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.336582</td>\n",
              "      <td>0.119698</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.269266</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.134633</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.107496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.134633</td>\n",
              "      <td>0.134633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.107496</td>\n",
              "      <td>0.201949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.053748</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.145768</td>\n",
              "      <td>0.145768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.201949</td>\n",
              "      <td>0.201949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.284977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050673</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.050673</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.354711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.202692</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.227982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113991</td>\n",
              "      <td>0.050673</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.101346</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.091014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045507</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045507</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050673</td>\n",
              "      <td>0.136522</td>\n",
              "      <td>0.091014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.123419</td>\n",
              "      <td>0.041140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041140</td>\n",
              "      <td>0.041140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 161 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     accord    accuse    action  ...      work      york  york times\n",
              "0  0.000000  0.000000  0.046912  ...  0.046912  0.000000    0.000000\n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.000000\n",
              "2  0.000000  0.052722  0.000000  ...  0.000000  0.105444    0.105444\n",
              "3  0.067316  0.067316  0.067316  ...  0.000000  0.201949    0.201949\n",
              "4  0.284977  0.000000  0.000000  ...  0.000000  0.000000    0.000000\n",
              "\n",
              "[5 rows x 161 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MoJl89piNqG",
        "outputId": "f922b8eb-5904-42aa-e309-c1e91d158c16"
      },
      "source": [
        "dtm_tfidf_tuned.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 161)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "AAlOas4giNnR",
        "outputId": "e344f391-f1d1-4a9f-cb56-e689d8f950f0"
      },
      "source": [
        "tfidf_vect = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "\n",
        "# Fit 후 dtm을 만듭니다.(문서, 단어마다 tf-idf 값을 계산합니다)\n",
        "dtm_tfidf_news = tfidf_vect.fit_transform(df['text'])\n",
        "\n",
        "dtm_tfidf_news = pd.DataFrame(dtm_tfidf_news.todense(), columns=tfidf_vect.get_feature_names())\n",
        "dtm_tfidf_news"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>2016</th>\n",
              "      <th>according</th>\n",
              "      <th>administration</th>\n",
              "      <th>america</th>\n",
              "      <th>american</th>\n",
              "      <th>americans</th>\n",
              "      <th>asked</th>\n",
              "      <th>called</th>\n",
              "      <th>campaign</th>\n",
              "      <th>china</th>\n",
              "      <th>city</th>\n",
              "      <th>clinton</th>\n",
              "      <th>committee</th>\n",
              "      <th>congress</th>\n",
              "      <th>country</th>\n",
              "      <th>court</th>\n",
              "      <th>day</th>\n",
              "      <th>democratic</th>\n",
              "      <th>democrats</th>\n",
              "      <th>department</th>\n",
              "      <th>did</th>\n",
              "      <th>don</th>\n",
              "      <th>donald</th>\n",
              "      <th>election</th>\n",
              "      <th>federal</th>\n",
              "      <th>foreign</th>\n",
              "      <th>friday</th>\n",
              "      <th>going</th>\n",
              "      <th>government</th>\n",
              "      <th>group</th>\n",
              "      <th>hillary</th>\n",
              "      <th>house</th>\n",
              "      <th>image</th>\n",
              "      <th>including</th>\n",
              "      <th>just</th>\n",
              "      <th>know</th>\n",
              "      <th>law</th>\n",
              "      <th>like</th>\n",
              "      <th>make</th>\n",
              "      <th>...</th>\n",
              "      <th>public</th>\n",
              "      <th>republican</th>\n",
              "      <th>republicans</th>\n",
              "      <th>reuters</th>\n",
              "      <th>right</th>\n",
              "      <th>russia</th>\n",
              "      <th>russian</th>\n",
              "      <th>said</th>\n",
              "      <th>say</th>\n",
              "      <th>saying</th>\n",
              "      <th>says</th>\n",
              "      <th>secretary</th>\n",
              "      <th>security</th>\n",
              "      <th>senate</th>\n",
              "      <th>state</th>\n",
              "      <th>statement</th>\n",
              "      <th>states</th>\n",
              "      <th>support</th>\n",
              "      <th>tax</th>\n",
              "      <th>think</th>\n",
              "      <th>thursday</th>\n",
              "      <th>time</th>\n",
              "      <th>told</th>\n",
              "      <th>trump</th>\n",
              "      <th>tuesday</th>\n",
              "      <th>twitter</th>\n",
              "      <th>united</th>\n",
              "      <th>vote</th>\n",
              "      <th>want</th>\n",
              "      <th>war</th>\n",
              "      <th>washington</th>\n",
              "      <th>way</th>\n",
              "      <th>wednesday</th>\n",
              "      <th>week</th>\n",
              "      <th>white</th>\n",
              "      <th>women</th>\n",
              "      <th>work</th>\n",
              "      <th>world</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100575</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045557</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110597</td>\n",
              "      <td>0.042812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049596</td>\n",
              "      <td>0.284336</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042666</td>\n",
              "      <td>0.0477</td>\n",
              "      <td>0.034209</td>\n",
              "      <td>0.045191</td>\n",
              "      <td>0.160846</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093176</td>\n",
              "      <td>0.081828</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049987</td>\n",
              "      <td>0.255902</td>\n",
              "      <td>0.425183</td>\n",
              "      <td>0.031260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.190021</td>\n",
              "      <td>0.044146</td>\n",
              "      <td>0.095490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.167187</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040103</td>\n",
              "      <td>0.048030</td>\n",
              "      <td>0.538793</td>\n",
              "      <td>0.048767</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037807</td>\n",
              "      <td>0.216284</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050922</td>\n",
              "      <td>0.041953</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100209</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083214</td>\n",
              "      <td>0.046387</td>\n",
              "      <td>0.101548</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043996</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051551</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.131273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.539628</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049684</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048887</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045941</td>\n",
              "      <td>0.243594</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.236661</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.036709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230136</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055825</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043904</td>\n",
              "      <td>0.054278</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053640</td>\n",
              "      <td>0.045768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033545</td>\n",
              "      <td>0.049645</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.280375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058103</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.084082</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.331559</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.054643</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133944</td>\n",
              "      <td>0.049777</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049247</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063567</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.120632</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.324988</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060814</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.132262</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.216814</td>\n",
              "      <td>0.064308</td>\n",
              "      <td>0.063207</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055879</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060578</td>\n",
              "      <td>0.098338</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.153468</td>\n",
              "      <td>0.063747</td>\n",
              "      <td>0.037494</td>\n",
              "      <td>0.110980</td>\n",
              "      <td>0.562291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.227917</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133686</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058493</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.046991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.518833</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.120193</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099809</td>\n",
              "      <td>0.111275</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055045</td>\n",
              "      <td>0.052769</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046789</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.177079</td>\n",
              "      <td>0.076849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.077351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.377219</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.084706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.165715</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071279</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.057150</td>\n",
              "      <td>0.075498</td>\n",
              "      <td>0.179143</td>\n",
              "      <td>0.176077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.084377</td>\n",
              "      <td>0.205456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154605</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071253</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.293697</td>\n",
              "      <td>0.103886</td>\n",
              "      <td>0.119046</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079765</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063161</td>\n",
              "      <td>0.516187</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.069510</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220501</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065170</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064341</td>\n",
              "      <td>0.279229</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056210</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051976</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059163</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.041531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065091</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.126316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056175</td>\n",
              "      <td>0.046171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059560</td>\n",
              "      <td>0.048287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037951</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.201857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.163529</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.262577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.061821</td>\n",
              "      <td>0.152798</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.151537</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053412</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060671</td>\n",
              "      <td>0.284153</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44893</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.140925</td>\n",
              "      <td>0.244637</td>\n",
              "      <td>0.267438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.123116</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.202257</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.246040</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063161</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.136409</td>\n",
              "      <td>0.143978</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.205959</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.213275</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.104177</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.223114</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110637</td>\n",
              "      <td>0.123346</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.122034</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.265776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44894</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.251346</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.417148</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.262100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.239829</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.215802</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.277695</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.224349</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44895</th>\n",
              "      <td>0.059549</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.051426</td>\n",
              "      <td>0.056219</td>\n",
              "      <td>0.055107</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030537</td>\n",
              "      <td>0.028740</td>\n",
              "      <td>0.050931</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032356</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032586</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047862</td>\n",
              "      <td>0.031722</td>\n",
              "      <td>0.054480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030819</td>\n",
              "      <td>0.047699</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.274439</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051729</td>\n",
              "      <td>0.106292</td>\n",
              "      <td>0.054319</td>\n",
              "      <td>0.027423</td>\n",
              "      <td>0.222328</td>\n",
              "      <td>0.074027</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083824</td>\n",
              "      <td>0.023841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.103441</td>\n",
              "      <td>0.131024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.148060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027495</td>\n",
              "      <td>0.031151</td>\n",
              "      <td>0.324714</td>\n",
              "      <td>0.027380</td>\n",
              "      <td>0.134500</td>\n",
              "      <td>0.026847</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027260</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.131397</td>\n",
              "      <td>0.063399</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028464</td>\n",
              "      <td>0.046902</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.112029</td>\n",
              "      <td>0.095259</td>\n",
              "      <td>0.441888</td>\n",
              "      <td>0.129645</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128265</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102756</td>\n",
              "      <td>0.144078</td>\n",
              "      <td>0.111739</td>\n",
              "      <td>0.021805</td>\n",
              "      <td>0.146757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44896</th>\n",
              "      <td>0.110016</td>\n",
              "      <td>0.109462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.407242</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.112835</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113877</td>\n",
              "      <td>0.176245</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.235649</td>\n",
              "      <td>0.100355</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.091180</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079988</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.080918</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105174</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.095807</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106473</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.080570</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44897</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.373877</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.225790</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074055</td>\n",
              "      <td>0.073408</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.069594</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.089626</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.146921</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.082222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066637</td>\n",
              "      <td>0.075788</td>\n",
              "      <td>0.150433</td>\n",
              "      <td>0.061822</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064655</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.081256</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.075204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038611</td>\n",
              "      <td>0.071762</td>\n",
              "      <td>0.155225</td>\n",
              "      <td>0.083389</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125906</td>\n",
              "      <td>0.079622</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.191056</td>\n",
              "      <td>0.061457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184681</td>\n",
              "      <td>0.067634</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063412</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44898 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            000      2016  according  ...     world      year     years\n",
              "0      0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.131273\n",
              "1      0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
              "2      0.000000  0.063567   0.000000  ...  0.000000  0.046789  0.000000\n",
              "3      0.000000  0.177079   0.076849  ...  0.000000  0.065170  0.000000\n",
              "4      0.000000  0.064341   0.279229  ...  0.060671  0.284153  0.000000\n",
              "...         ...       ...        ...  ...       ...       ...       ...\n",
              "44893  0.000000  0.140925   0.244637  ...  0.265776  0.000000  0.000000\n",
              "44894  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
              "44895  0.059549  0.029624   0.051426  ...  0.111739  0.021805  0.146757\n",
              "44896  0.110016  0.109462   0.000000  ...  0.000000  0.080570  0.000000\n",
              "44897  0.000000  0.000000   0.373877  ...  0.000000  0.063412  0.000000\n",
              "\n",
              "[44898 rows x 100 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DupYYFkjOSq",
        "outputId": "c3ff155a-44ae-4915-ec31-610547b95544"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# dtm을 사용히 NN 모델을 학습시킵니다. (디폴트)최근접 5 이웃.\n",
        "nn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
        "nn.fit(dtm_tfidf_news)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfdHePcbjQof",
        "outputId": "fd3ed8fd-592d-4a65-a185-25127d65a105"
      },
      "source": [
        "nn.kneighbors([dtm_tfidf_news.iloc[42]])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.38516929, 0.46109983, 0.57760059, 0.65143189]]),\n",
              " array([[  42,   23,    1,  214, 1463]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghQSPbQE7P8b"
      },
      "source": [
        "## 2.2 Keras Embedding을 사용하여 분류하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEl8iU6j8I4M"
      },
      "source": [
        "### 2.2.0 데이터셋 split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJYxNZGuUBEI"
      },
      "source": [
        "- Train, Test 데이터셋으로 분리(Split)하여 주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55XSZyY_Sj54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "1f6fb3f7-4031-4efc-d954-f11fe52f19b5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "fake = pd.read_csv('/content/drive/MyDrive/Fake.csv')\n",
        "true = pd.read_csv('/content/drive/MyDrive/True.csv')\n",
        "\n",
        "true['label'] = [0 for _ in range(true.shape[0])]\n",
        "fake['label'] = [0 for _ in range(fake.shape[0])]\n",
        "\n",
        "df = pd.concat([true, fake])\n",
        "\n",
        "true.shape, fake.shape, df.shape\n",
        "\n",
        "train, test = train_test_split(df, train_size=0.85, test_size=0.15, \n",
        "                              stratify=df['label'], random_state=2)\n",
        "\n",
        "X_train, y_train, X_test, y_test = train.drop('label', axis = 1), train.label, test.drop('label', axis = 1), test.label\n",
        "\n",
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 300  # Only consider the first 300 words of each movie review\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# TF-IDF vectorizer. 테이블을 작게 만들기 위해 max_features=15로 제한하였습니다.\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=15)\n",
        "\n",
        "# Fit 후 dtm을 만듭니다.(문서, 단어마다 tf-idf 값을 계산합니다)\n",
        "X_train = tfidf.fit_transform(df.title)\n",
        "X_train = pd.DataFrame(X_train.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "X_test = tfidf.fit_transform(df.title)\n",
        "X_test = pd.DataFrame(X_test.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "len(X_train), len(X_test)\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train.values, maxlen=maxlen)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test.values, maxlen=maxlen)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2958fa2ef673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m    152\u001b[0m   return sequence.pad_sequences(\n\u001b[1;32m    153\u001b[0m       \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m keras_export(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'clinton'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1xXxSMn7fyt"
      },
      "source": [
        "### 2.2.1 단어 벡터의 평균을 이용하여 분류해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_ZsjAVg7ndG"
      },
      "source": [
        "N422에서 했던 단어 임베딩 벡터의 평균을 사용하여 문장을 분류하는 작업을 수행해봅시다.<br/>\n",
        "인스턴스마다 텍스트 길이가 길고 시간이 오래 걸리므로 시간상 epoch 수를 **10 이하**로 하는 것을 추천드립니다.<br/>\n",
        "모델 구동이 목적이므로 임베딩 차원 수를 크지 않게(50이하)로 설정해주세요.<br/>\n",
        "**권장사항 : `max_len` 은 텍스트 길이 평균보다 높게 설정해주세요.**<br/>\n",
        "\n",
        "> **Tip : 모델이 학습하는 동안 2.2.3의 내용을 작성하면 시간을 절약할 수 있습니다.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHber3qBBwOl"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model2 = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "POGI4q2PmTLN",
        "outputId": "1b7335a2-2388-4ee5-f061-6e4452342b94"
      },
      "source": [
        "model2.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model2.fit(\n",
        "    X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test)\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b4cca2697be4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model2.fit(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1647\u001b[0m           label, \", \".join(str(i.shape[0]) for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1648\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 44898\n  y sizes: 38163\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SzUwLkcAK1A"
      },
      "source": [
        "### 2.2.2 LSTM을 사용하여 텍스트 분류 수행해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4UZ9ZqOAIjw"
      },
      "source": [
        "N423에서 했던 단어 임베딩 벡터의 평균을 사용하여 문장을 분류하는 작업을 수행해봅시다.<br/>\n",
        "인스턴스마다 텍스트 길이가 길어 시간이 매우 오래 걸리므로 <br/>\n",
        "**층을 최소한으로 쌓고**, epoch 수를 **3 이하**로 하는 것을 추천드립니다.<br/>\n",
        "\n",
        "> **Tip : 모델이 학습하는 동안 2.2.3의 내용을 작성하면 시간을 절약할 수 있습니다.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PLNlzEVBSyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d7007a-bd81-42b8-8d11-f4b378362c6c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 파라미터를 설정합니다.\n",
        "max_features = 20000\n",
        "maxlen = 300\n",
        "batch_size = 32\n",
        "\n",
        "# model을 정의합니다.\n",
        "\"\"\"\n",
        "keras의 기본 Embedding 벡터를 사용하였으며\n",
        "LSTM 층에 dropout/recurrent_dropout을 적용하였습니다.\n",
        "\"\"\"\n",
        "model3 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(max_features, 128),\n",
        "  tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model3.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model3.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "t1W3zVPBsCDR",
        "outputId": "31c051be-008f-4be5-9dcd-ec5a671df9c6"
      },
      "source": [
        "unicorns = model3.fit(X_train, y_train,\n",
        "          batch_size=batch_size, \n",
        "          epochs=3, \n",
        "          validation_data=(X_test,y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-f7385a21f9c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           validation_data=(X_test,y_test))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node sequential_2/Cast (defined at <ipython-input-22-f7385a21f9c1>:4) ]] [Op:__inference_train_function_6846]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "tQz12auys4xt",
        "outputId": "56ca8037-c0bd-438c-9f4e-30a6f8f0e66d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-cb4b5b64d6fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot training & validation loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municorns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municorns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'unicorns' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsNOs2wmBV1z"
      },
      "source": [
        "### 2.2.3 위에서 실행한 내용에 대해 다시 알아봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMR2t2wOEPYm"
      },
      "source": [
        "#### a) 데이터셋을 학습할 때 사용하는 `pad_sequences`  메서드에 대해 설명해주세요.<br/>어떤 기능을 하나요? 모델을 학습할 때 왜 필요한가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_ECWSYQ4JV"
      },
      "source": [
        "가변 시퀀스들을 채우기 위해서 사용하며, 부자연스러운 시퀀스 문제를 해결하기 위해서 사용한다. \n",
        "기계는 전부 동일한 길이의 문서에 대해서는 하나의 행렬로 보고 묶어서 연산 할 수 있기 때문에 pad_sequences로 길이를 일정하게 맞춰서 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeZ6AcloEYD5"
      },
      "source": [
        "#### b) 2.2.1과 2.2.2에서 사용한 각 모델의 evaluation 성능은 어떻게 나왔나요?<br/>각 모델의 장단점은 무엇이라고 생각하나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SyqQzzHQ4EP"
      },
      "source": [
        "TF-IDF는 단기 의존성 문제를 가지지만 LSTM은 단기의존성 문제를 해결하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUwKojAGM82f"
      },
      "source": [
        "#### c) 종래의 RNN(Recurrent Neural Networks) 대신 LSTM(Long-Short Term Memory)을 사용하는 이유는 무엇인가요?<br/>(i.e. RNN에 비해 LSTM의 좋은 점을 설명해주세요.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y44XfugQ_HR"
      },
      "source": [
        "RNN은 앞쪽 시퀀스의 데이터가 거의 지워져 단기 의존성 문제가 있는데 이를 해결하기위해 LSTM을 사용 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J1kzTmDEaLU"
      },
      "source": [
        "#### d) LSTM이나 RNN을 사용하는 예시를 **3개**이상 제시하고 해당되는 경우에 왜 LSTM이나 RNN을 사용하는 것 적절한지 간단하게 설명해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck8GPjc_Q_vA"
      },
      "source": [
        "이미지 켑셔닝, 감성 분석, 기계번역, 비디오 프레임별로 분류 하는데 사용되며 시퀀셜한 데이터들을 읽기에 LSTM과 RNN이 좋아서 사용 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4pP7g8DE0Cz"
      },
      "source": [
        "#### e) 이외에 N424 에서 배운 자연어처리 모델과 관련된 키워드를 3개 이상 적어주세요. <br/> (해당 키워드에 대한 설명은 옵션입니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-sO4mMuRAhp"
      },
      "source": [
        "Transformer : 기계번역을 위한 새로운 모델로 등장한 Attention 매커니즘을 극대화하여 뛰어난 번역성능을 가진 모델로 RNN을 사용하지 않아 병렬화가 되서 빠르고 효과가 좋다. GPU 연산에 최적화 되어 있다.\n",
        "\n",
        "GPT : 트렌스포머의 디코더 기반 모델이다. 비지도 학습이며 다양하고 엄청난 양의 데이터를 통해서 프리 트레이닝 된다.\n",
        "\n",
        "BERT : 양쪽 문맥을 동시에 탐색한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRn44qjhUo9j"
      },
      "source": [
        "# Advanced Goals: 3점을 획득하기 위해선 아래의 조건 중 하나 이상을 만족해야합니다\n",
        " \n",
        "- 2.1 에서 TF-IDF(`TfidfVectorizer`)가 아닌 방법을 사용하여 유사도 검색을 수행해보세요.<br/>\n",
        "TF-IDF와 해당 방법의 차이를 설명해주세요. \n",
        "- 2.2 에서 사용한 방법을 재사용하되 하이퍼 파라미터를 조정하거나 모델 구조를 변경하여 성능을 올려봅시다.<br/>**(주의 : GridSearch, RandomSearch 등의 방법을 사용하여도 좋으나 시간이 오래 걸리므로 범위를 잘 선택해야 합니다.)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz2uaXFYUo9j"
      },
      "source": [
        "# 이 곳에 답안을 작성하시길 바랍니다"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}