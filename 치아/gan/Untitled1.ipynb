{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    Define dataset for training GAN\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, z_input_dim):\n",
    "        # load mnist dataset\n",
    "        # 이미지는 보통 -1~1 사이의 값으로 normalization : generator의 outputlayer를 tanh로\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "        self.x_data = ((X_train.astype(np.float32) - 127.5) / 127.5)\n",
    "        self.x_data = self.x_data.reshape((self.x_data.shape[0], 1) + self.x_data.shape[1:])\n",
    "        self.batch_size = batch_size\n",
    "        self.z_input_dim = z_input_dim\n",
    "\n",
    "    def get_real_sample(self):\n",
    "        \"\"\"\n",
    "        get real sample mnist images\n",
    "\n",
    "        :return: batch_size number of mnist image data\n",
    "        \"\"\"\n",
    "        return self.x_data[np.random.randint(0, self.x_data.shape[0], size=self.batch_size)]\n",
    "\n",
    "    def get_z_sample(self, sample_size):\n",
    "        \"\"\"\n",
    "        get z sample data\n",
    "\n",
    "        :return: random z data (batch_size, z_input_dim) size\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-1.0, 1.0, (sample_size, self.z_input_dim))\n",
    "\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self, learning_rate, z_input_dim):\n",
    "        \"\"\"\n",
    "        init params\n",
    "\n",
    "        :param learning_rate: learning rate of optimizer\n",
    "        :param z_input_dim: input dim of z\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.z_input_dim = z_input_dim\n",
    "        self.D = self.discriminator()\n",
    "        self.G = self.generator()\n",
    "        self.GD = self.combined()\n",
    "\n",
    "    def discriminator(self):\n",
    "        \"\"\"\n",
    "        define discriminator\n",
    "        \"\"\"\n",
    "        D = Sequential()\n",
    "        D.add(Conv2D(256, (5, 5),\n",
    "                     padding='same',\n",
    "                     input_shape=(1, 28, 28),\n",
    "                     kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "        D.add(LeakyReLU(0.2))\n",
    "        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        D.add(Dropout(0.3))\n",
    "        D.add(Conv2D(512, (5, 5), padding='same'))\n",
    "        D.add(LeakyReLU(0.2))\n",
    "        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        D.add(Dropout(0.3))\n",
    "        D.add(Flatten())\n",
    "        D.add(Dense(256))\n",
    "        D.add(LeakyReLU(0.2))\n",
    "        D.add(Dropout(0.3))\n",
    "        D.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n",
    "        D.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        return D\n",
    "\n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        define generator\n",
    "        \"\"\"\n",
    "        G = Sequential()\n",
    "        G.add(Dense(512, input_dim=self.z_input_dim))\n",
    "        G.add(LeakyReLU(0.2))\n",
    "        G.add(Dense(128 * 7 * 7))\n",
    "        G.add(LeakyReLU(0.2))\n",
    "        G.add(BatchNormalization())\n",
    "        G.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))\n",
    "        G.add(UpSampling2D(size=(2, 2)))\n",
    "        G.add(Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "        G.add(UpSampling2D(size=(2, 2)))\n",
    "        G.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n",
    "        G.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        return G\n",
    "\n",
    "    def combined(self):\n",
    "        \"\"\"\n",
    "        defien combined gan model\n",
    "        \"\"\"\n",
    "        G, D = self.G, self.D\n",
    "        D.trainable = False\n",
    "        GD = Sequential()\n",
    "        GD.add(G)\n",
    "        GD.add(D)\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n",
    "        GD.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        D.trainable = True\n",
    "        return GD\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, args):\n",
    "        self.epochs = args.epochs\n",
    "        self.batch_size = args.batch_size\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.z_input_dim = args.z_input_dim\n",
    "        self.data = Data(self.batch_size, self.z_input_dim)\n",
    "\n",
    "        # the reason why D, G differ in iter : Generator needs more training than Discriminator\n",
    "        self.n_iter_D = args.n_iter_D\n",
    "        self.n_iter_G = args.n_iter_G\n",
    "        self.gan = GAN(self.learning_rate, self.z_input_dim)\n",
    "        self.d_loss = []\n",
    "        self.g_loss = []\n",
    "\n",
    "        # print status\n",
    "        batch_count = self.data.x_data.shape[0] / self.batch_size\n",
    "        print('Epochs:', self.epochs)\n",
    "        print('Batch size:', self.batch_size)\n",
    "        print('Batches per epoch:', batch_count)\n",
    "        print('Learning rate:', self.learning_rate)\n",
    "        print('Image data format:', K.image_data_format())\n",
    "\n",
    "    def fit(self):\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # train discriminator by real data\n",
    "            dloss = 0\n",
    "            for iter in range(self.n_iter_D):\n",
    "                dloss = self.train_D()\n",
    "\n",
    "            # train GD by generated fake data\n",
    "            gloss = 0\n",
    "            for iter in range(self.n_iter_G):\n",
    "                gloss = self.train_G()\n",
    "\n",
    "            # print loss data\n",
    "            print('Discriminator loss:', str(dloss))\n",
    "            print('Generator loss:', str(gloss))\n",
    "\n",
    "    def train_D(self):\n",
    "        \"\"\"\n",
    "        train Discriminator\n",
    "        \"\"\"\n",
    "\n",
    "        # Real data\n",
    "        real = self.data.get_real_sample()\n",
    "\n",
    "        # Generated data\n",
    "        z = self.data.get_z_sample(self.batch_size)\n",
    "        generated_images = self.gan.G.predict(z)\n",
    "\n",
    "        # labeling and concat generated, real images\n",
    "        x = np.concatenate((real, generated_images), axis=0)\n",
    "        y = [0.9] * self.batch_size + [0] * self.batch_size\n",
    "\n",
    "        # train discriminator\n",
    "        self.gan.D.trainable = True\n",
    "        loss = self.gan.D.train_on_batch(x, y)\n",
    "        return loss\n",
    "\n",
    "    def train_G(self):\n",
    "        \"\"\"\n",
    "        train Generator\n",
    "        \"\"\"\n",
    "\n",
    "        # Generated data\n",
    "        z = self.data.get_z_sample(self.batch_size)\n",
    "\n",
    "        # labeling\n",
    "        y = [1] * self.batch_size\n",
    "\n",
    "        # train generator\n",
    "        self.gan.D.trainable = False\n",
    "        loss = self.gan.GD.train_on_batch(z, y)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--z_input_dim Z_INPUT_DIM] [--n_iter_D N_ITER_D]\n",
      "                             [--n_iter_G N_ITER_G]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\forev\\AppData\\Roaming\\jupyter\\runtime\\kernel-88158548-005a-4b49-b037-183fe7ffcc46.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # set hyper parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='Batch size for networks')\n",
    "    parser.add_argument('--epochs', type=int, default=200,\n",
    "                        help='Epochs for the networks')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0002,\n",
    "                        help='Learning rate')\n",
    "    parser.add_argument('--z_input_dim', type=int, default=100,\n",
    "                        help='Input dimension for the generator.')\n",
    "    parser.add_argument('--n_iter_D', type=int, default=1,\n",
    "                        help='training iteration for D')\n",
    "    parser.add_argument('--n_iter_G', type=int, default=5,\n",
    "                        help='training iteration for G')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # run model\n",
    "    model = Model(args)\n",
    "    model.fit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
