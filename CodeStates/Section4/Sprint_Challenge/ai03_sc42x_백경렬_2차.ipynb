{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ai03-sc42x-백경렬.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernel_info": {
      "name": "u4-s1-nlp"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "nteract": {
      "version": "0.22.4"
    },
    "toc-autonumbering": false
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy_9hARVUo9L"
      },
      "source": [
        "# SC42x \n",
        "## 자연어처리 (Natural Language Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dNUXHiLbBdp"
      },
      "source": [
        "# Part 1 : 개념 요약\n",
        "\n",
        "> 다음의 키워드에 대해서 **한 줄**로 간단하게 요약해주세요. (세션 노트를 참고하여도 좋습니다.)<br/>\n",
        "> **Tip : 아래 문제를 먼저 수행한 후 모델 학습 등 시간이 오래 걸리는 셀이 실행되는 동안 아래 내용을 작성하면 시간을 절약할 수 있습니다.**\n",
        "\n",
        "**N421**\n",
        "- Stopwords(불용어) : 분석할 때 의미가 없는 단어 예를 들어 제품 리뷰를 이해 하는데 I, and, of 같은 단어들은 의미가 없는데 이들을 불용어라고 한다.\n",
        "- Stemming과 Lemmatization : 어간 추출(Stemming)이란 단어의 의미가 포함된 부분으로 접사등이 제거된 형태로 만드는 것으로 예를 들어 argue, argued, arguing, argus의 뒷부분인 ing, ed, s를 제거 하는 과정이며 argu라는 어간을 뽑아내는 과정이며,\n",
        "표제어 추출(Lemmatization)이란 어간 추출보다 체계적으로 단어들의 기본 사전형 단어 형태인 표제어로 변환하는 과정이며 예를 들어 명사의 복수형은 단수형으로, 동사는 모두 타동사로 변환하는 과정을 말한다.\n",
        "- Bag-of-Words : 가장 단순한 벡터화 방법 중 하나로 문서 혹은 문장에서 문법이나 단어의 순서 등을 무시하고 단순히 단어들의 빈도만 고려하여 벡터화 하는 과정\n",
        "- TF-IDF : 전체적인 문서군에서 자주 등장하는 단어들은 DF ( 문서 빈도 ) 라고 하며, 이 값의 역수를 IDF ( 역문서 빈도 ) 라 하고, 그리고 그 값과 TF ( 단어 빈도 ) 값을 곱해 준 값이 TF-IDF 라고 한다.\n",
        "\n",
        "**N422**\n",
        "- Word2Vec : 단어를 벡터로 나타내는 가장 널리 쓰이는 임베딩 방법 중 하나.\n",
        "- fastText : Word2Vec 방식에 철자 (Character) 기반의 임베딩 방식을 더한 새로운 임베딩 방식이다.\n",
        "\n",
        "**N423**\n",
        "- RNN : 연속형 데이터들을 잘 처리하기 위해 고안된 신경망으로 기존 신경망에는 없던 출력 벡터가 다시 입력되는 특성 때문에 순환(Recurrent) 신경망 이라는 이름이 붙었다.\n",
        "- LSTM, GRU : LSTM은 Long-Short Term Memory로 장단기 기억망이라 하며 RNN의 기울기 정보 크기를 조절하고자 Gate를 3개 추가했으며, 내부의 Cell-state는 역전파 과정에서 활성화 함수를 거치지 않아 앞쪽 시퀀스의 정보를 어느정도 잃지 않게 해주는 모델이며, \n",
        "GRU는 Gate Recurrent Unit으로 LSTM의 간소화 버전이며, LSTM의 Output Gate가 없어졌으며, Cell-state의 벡터와 hidden-state 벡터가 통합되어 하나의 벡터로 출력된다.\n",
        "- Attention : 인코더에서 각 Time-step 마다 생성되는 hidden-state 벡터들을 모아서 디코더의 hidden-state 벡터와 내적한 값을 이용해 모든 단어가 영향력을 잃지 않게 해주는 과정이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g07_yjCgbBdp"
      },
      "source": [
        "# Part 2 : Fake/Real News Dataset\n",
        "\n",
        "한 주간 자연어처리 기법을 배우면서 여러분은 다양한 기술들을 접했습니다.<br/>\n",
        "어떻게 텍스트 데이터를 다뤄야 하는지, 텍스트를 벡터화 하는 법, 문서에서 토픽을 모델하는 법 등 다양한 NLP 기법을 배웠는데요.<br/>\n",
        "이번 스프린트 챌린지에선 [Fake/Real News Dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)을 사용하여 배운 것들을 복습해보는 시간을 갖겠습니다.\n",
        "\n",
        "**주의 : 모델의 성능을 최대한 끌어올리는 것이 아닌 모델 구동에 초점을 맞춰주세요.<br/>\n",
        "모든 문제를 완료한 후에도 \"시간이 남았다면\" 정확도를 올리는 것에 도전하시는 것을 추천드립니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCX5K0nP0ZKQ"
      },
      "source": [
        "# 코드 실행 전 seed를 지정하겠습니다.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C08-JiyNbdQy"
      },
      "source": [
        "## 2.0 데이터셋을 불러옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyndWa5hxEmk"
      },
      "source": [
        "- 위 캐글 링크에서 데이터셋을 받아 업로드 합니다.<br/>\n",
        "(직접 업로드하게 되면 시간이 꽤 걸리므로 **drive_mount** 나 **kaggle 연동**하시는 것을 추천드립니다.)\n",
        "\n",
        "- 'label' 열을 만들어 Fake = 1, True = 0 로 레이블링해줍니다.\n",
        "- 두 파일을 합쳐 하나의 데이터프레임에 저장해 준 후 데이터를 섞어줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uqDCdi5blfk",
        "outputId": "4fb2c96c-095b-4c29-b677-8dfd4c61fd0e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSUccDw1bf3r",
        "outputId": "d3fc5b3b-de32-488e-bda0-afd4102e94a1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "fake = pd.read_csv('/content/drive/MyDrive/Fake.csv')\n",
        "true = pd.read_csv('/content/drive/MyDrive/True.csv')\n",
        "\n",
        "true['label'] = [0 for _ in range(true.shape[0])]\n",
        "fake['label'] = [0 for _ in range(fake.shape[0])]\n",
        "\n",
        "df = pd.concat([true, fake], ignore_index=True)\n",
        "\n",
        "true.shape, fake.shape, df.shape"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21417, 5), (23481, 5), (44898, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbxEHBanUo9d"
      },
      "source": [
        "## 2.1 TF-IDF 를 활용하여 특정 뉴스와 유사한 뉴스 검색하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4R5mLMS5c2I"
      },
      "source": [
        "시간상 특별한 **전처리 없이** 아래 태스크를 수행하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "5i4RSybdgO62",
        "outputId": "5b1693cb-97b5-4e6e-a33a-66fbbfa257e3"
      },
      "source": [
        "df"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44893</th>\n",
              "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
              "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 16, 2016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44894</th>\n",
              "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
              "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 16, 2016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44895</th>\n",
              "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
              "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 15, 2016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44896</th>\n",
              "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
              "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 14, 2016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44897</th>\n",
              "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
              "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
              "      <td>Middle-east</td>\n",
              "      <td>January 12, 2016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44898 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ... label\n",
              "0      As U.S. budget fight looms, Republicans flip t...  ...     0\n",
              "1      U.S. military to accept transgender recruits o...  ...     0\n",
              "2      Senior U.S. Republican senator: 'Let Mr. Muell...  ...     0\n",
              "3      FBI Russia probe helped by Australian diplomat...  ...     0\n",
              "4      Trump wants Postal Service to charge 'much mor...  ...     0\n",
              "...                                                  ...  ...   ...\n",
              "44893  McPain: John McCain Furious That Iran Treated ...  ...     0\n",
              "44894  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...  ...     0\n",
              "44895  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...  ...     0\n",
              "44896  How to Blow $700 Million: Al Jazeera America F...  ...     0\n",
              "44897  10 U.S. Navy Sailors Held by Iranian Military ...  ...     0\n",
              "\n",
              "[44898 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2B-6Wkk5YGD"
      },
      "source": [
        "### 2.1.1 TFidfVectorizer를 사용하여 문서-단어 행렬(Document-Term Matrix) 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kw9OQwmUo9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "c19d830c-43e0-4a9b-b07d-fededbbc0766"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# TF-IDF vectorizer. 테이블을 작게 만들기 위해 max_features=15로 제한하였습니다.\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=15)\n",
        "\n",
        "# Fit 후 dtm을 만듭니다.(문서, 단어마다 tf-idf 값을 계산합니다)\n",
        "dtm_tfidf = tfidf.fit_transform(df.text)\n",
        "\n",
        "dtm_tfidf = pd.DataFrame(dtm_tfidf.todense(), columns=tfidf.get_feature_names())\n",
        "dtm_tfidf"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clinton</th>\n",
              "      <th>donald</th>\n",
              "      <th>government</th>\n",
              "      <th>house</th>\n",
              "      <th>new</th>\n",
              "      <th>obama</th>\n",
              "      <th>people</th>\n",
              "      <th>president</th>\n",
              "      <th>republican</th>\n",
              "      <th>reuters</th>\n",
              "      <th>said</th>\n",
              "      <th>state</th>\n",
              "      <th>states</th>\n",
              "      <th>trump</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071750</td>\n",
              "      <td>0.171626</td>\n",
              "      <td>0.515884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.140037</td>\n",
              "      <td>0.124764</td>\n",
              "      <td>0.536731</td>\n",
              "      <td>0.065566</td>\n",
              "      <td>0.398552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.084113</td>\n",
              "      <td>0.453636</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071524</td>\n",
              "      <td>0.085543</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.378701</td>\n",
              "      <td>0.139597</td>\n",
              "      <td>0.310928</td>\n",
              "      <td>0.089174</td>\n",
              "      <td>0.065360</td>\n",
              "      <td>0.546287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.646014</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.099359</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160666</td>\n",
              "      <td>0.145057</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.116568</td>\n",
              "      <td>0.250738</td>\n",
              "      <td>0.061259</td>\n",
              "      <td>0.372373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.847677</td>\n",
              "      <td>0.076444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.133781</td>\n",
              "      <td>0.090260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.324489</td>\n",
              "      <td>0.292965</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.088083</td>\n",
              "      <td>0.156952</td>\n",
              "      <td>0.112534</td>\n",
              "      <td>0.164962</td>\n",
              "      <td>0.188016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.815243</td>\n",
              "      <td>0.102927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081392</td>\n",
              "      <td>0.292037</td>\n",
              "      <td>0.097536</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.353827</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074377</td>\n",
              "      <td>0.395600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.190833</td>\n",
              "      <td>0.514601</td>\n",
              "      <td>0.556885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44893</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.194102</td>\n",
              "      <td>0.712426</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.311962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.124569</td>\n",
              "      <td>0.406204</td>\n",
              "      <td>0.420635</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44894</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44895</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.494734</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.522133</td>\n",
              "      <td>0.136887</td>\n",
              "      <td>0.235476</td>\n",
              "      <td>0.059941</td>\n",
              "      <td>0.042978</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.585366</td>\n",
              "      <td>0.242465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44896</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.704537</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.709667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44897</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154755</td>\n",
              "      <td>0.419163</td>\n",
              "      <td>0.683768</td>\n",
              "      <td>0.126025</td>\n",
              "      <td>0.449120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.089669</td>\n",
              "      <td>0.292398</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.147264</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44898 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        clinton    donald  government  ...    states     trump      year\n",
              "0      0.000000  0.071750    0.171626  ...  0.084113  0.453636  0.000000\n",
              "1      0.000000  0.071524    0.085543  ...  0.000000  0.646014  0.000000\n",
              "2      0.099359  0.000000    0.000000  ...  0.000000  0.847677  0.076444\n",
              "3      0.133781  0.090260    0.000000  ...  0.000000  0.815243  0.102927\n",
              "4      0.000000  0.081392    0.292037  ...  0.190833  0.514601  0.556885\n",
              "...         ...       ...         ...  ...       ...       ...       ...\n",
              "44893  0.000000  0.000000    0.000000  ...  0.420635  0.000000  0.000000\n",
              "44894  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000\n",
              "44895  0.000000  0.000000    0.494734  ...  0.242465  0.000000  0.039309\n",
              "44896  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.709667\n",
              "44897  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.147264\n",
              "\n",
              "[44898 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeTMNUMM5WgQ"
      },
      "source": [
        "### 2.1.2 KNN 알고리즘을 사용하여 유사한 문서 검색하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KjQvaVCUBEH"
      },
      "source": [
        "- **42번 인덱스의 문서**와 가장 유사한 **5개 문서(42번 포함)의 인덱스**와 **해당 인덱스의 레이블**을 나타내주세요.\n",
        "- NN 모델의 파라미터 중 `algorithm = 'kd_tree'` 로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBmsCLMlwP1p"
      },
      "source": [
        "# SpaCy 를 이용한 Tokenizing\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "def tokenize(document):\n",
        "    doc = nlp(document)\n",
        "    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True) and (token.is_alpha == True)]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jjaQO8O6UKd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "1d371477-2956-4226-d9d7-49476f662443"
      },
      "source": [
        "    \"\"\"\n",
        "    args:\n",
        "        ngram_range = (min_n, max_n), min_n 개~ max_n 개를 갖는 n-gram(n개의 연속적인 토큰)을 토큰으로 사용합니다.\n",
        "        min_df = n : int, 최소 n개의 문서에 나타나는 토큰만 사용합니다.\n",
        "        max_df = m : float(0~1), m * 100% 이상 문서에 나타나는 토큰은 제거합니다.\n",
        "    \"\"\"\n",
        "\n",
        "tfidf_tuned = TfidfVectorizer(stop_words='english'\n",
        "                        ,tokenizer=tokenize\n",
        "                        ,ngram_range=(1,2)\n",
        "                        ,max_df=.7\n",
        "                        ,min_df=3\n",
        "                       )\n",
        "\n",
        "dtm_tfidf_tuned = tfidf_tuned.fit_transform(df['text'].iloc[0:10])\n",
        "dtm_tfidf_tuned = pd.DataFrame(dtm_tfidf_tuned.todense(), columns=tfidf_tuned.get_feature_names())\n",
        "dtm_tfidf_tuned.head()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accord</th>\n",
              "      <th>accuse</th>\n",
              "      <th>action</th>\n",
              "      <th>administration</th>\n",
              "      <th>agreement</th>\n",
              "      <th>ally</th>\n",
              "      <th>american</th>\n",
              "      <th>april</th>\n",
              "      <th>base</th>\n",
              "      <th>begin</th>\n",
              "      <th>bias</th>\n",
              "      <th>bias republican</th>\n",
              "      <th>billion</th>\n",
              "      <th>california</th>\n",
              "      <th>campaign</th>\n",
              "      <th>charge</th>\n",
              "      <th>charge trump</th>\n",
              "      <th>child</th>\n",
              "      <th>clinton</th>\n",
              "      <th>collusion</th>\n",
              "      <th>come</th>\n",
              "      <th>comment</th>\n",
              "      <th>congress</th>\n",
              "      <th>conservative</th>\n",
              "      <th>continue</th>\n",
              "      <th>counsel</th>\n",
              "      <th>counsel robert</th>\n",
              "      <th>country</th>\n",
              "      <th>defense</th>\n",
              "      <th>democratic</th>\n",
              "      <th>democrats</th>\n",
              "      <th>deny</th>\n",
              "      <th>department</th>\n",
              "      <th>despite</th>\n",
              "      <th>dollar</th>\n",
              "      <th>early</th>\n",
              "      <th>edit</th>\n",
              "      <th>election</th>\n",
              "      <th>end</th>\n",
              "      <th>expect</th>\n",
              "      <th>...</th>\n",
              "      <th>secretary</th>\n",
              "      <th>senate</th>\n",
              "      <th>senator</th>\n",
              "      <th>september</th>\n",
              "      <th>set</th>\n",
              "      <th>small</th>\n",
              "      <th>speak</th>\n",
              "      <th>special</th>\n",
              "      <th>special counsel</th>\n",
              "      <th>spend</th>\n",
              "      <th>spending</th>\n",
              "      <th>statement</th>\n",
              "      <th>states</th>\n",
              "      <th>supporter</th>\n",
              "      <th>talk</th>\n",
              "      <th>tax</th>\n",
              "      <th>team</th>\n",
              "      <th>tell</th>\n",
              "      <th>think</th>\n",
              "      <th>time</th>\n",
              "      <th>times</th>\n",
              "      <th>trump administration</th>\n",
              "      <th>trump ally</th>\n",
              "      <th>trump say</th>\n",
              "      <th>twitt</th>\n",
              "      <th>twitter</th>\n",
              "      <th>united</th>\n",
              "      <th>united states</th>\n",
              "      <th>use</th>\n",
              "      <th>want</th>\n",
              "      <th>washington</th>\n",
              "      <th>washington reuters</th>\n",
              "      <th>way</th>\n",
              "      <th>wednesday</th>\n",
              "      <th>week</th>\n",
              "      <th>white</th>\n",
              "      <th>white house</th>\n",
              "      <th>work</th>\n",
              "      <th>york</th>\n",
              "      <th>york times</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.083416</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083416</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093824</td>\n",
              "      <td>0.140736</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.140736</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>0.23456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.125125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.23456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.375296</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.187648</td>\n",
              "      <td>0.067723</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.093824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.033861</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.572920</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.171876</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.114584</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128881</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046513</td>\n",
              "      <td>0.257761</td>\n",
              "      <td>0.051451</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.257761</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051451</td>\n",
              "      <td>...</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128881</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.139540</td>\n",
              "      <td>0.046513</td>\n",
              "      <td>0.057292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.114584</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.210888</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.316332</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.152220</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042095</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.046874</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.042095</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.052722</td>\n",
              "      <td>0.158166</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046874</td>\n",
              "      <td>0.158166</td>\n",
              "      <td>0.076110</td>\n",
              "      <td>0.038055</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093747</td>\n",
              "      <td>0.038055</td>\n",
              "      <td>0.038055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105444</td>\n",
              "      <td>0.105444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.336582</td>\n",
              "      <td>0.119698</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.269266</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.134633</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.107496</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.134633</td>\n",
              "      <td>0.134633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.059849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.107496</td>\n",
              "      <td>0.201949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067316</td>\n",
              "      <td>0.053748</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.048589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.145768</td>\n",
              "      <td>0.145768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.201949</td>\n",
              "      <td>0.201949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.284977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050673</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.050673</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.354711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.202692</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.227982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113991</td>\n",
              "      <td>0.050673</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.101346</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.091014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045507</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045507</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050673</td>\n",
              "      <td>0.136522</td>\n",
              "      <td>0.091014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.123419</td>\n",
              "      <td>0.041140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041140</td>\n",
              "      <td>0.041140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 161 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     accord    accuse    action  ...      work      york  york times\n",
              "0  0.000000  0.000000  0.046912  ...  0.046912  0.000000    0.000000\n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.000000\n",
              "2  0.000000  0.052722  0.000000  ...  0.000000  0.105444    0.105444\n",
              "3  0.067316  0.067316  0.067316  ...  0.000000  0.201949    0.201949\n",
              "4  0.284977  0.000000  0.000000  ...  0.000000  0.000000    0.000000\n",
              "\n",
              "[5 rows x 161 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MoJl89piNqG",
        "outputId": "cca9eb4f-471b-4d9d-c5e1-3d1406b9f98b"
      },
      "source": [
        "dtm_tfidf_tuned.shape"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 161)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "AAlOas4giNnR",
        "outputId": "6250428a-1e56-4465-a4b7-55af17ea2d0c"
      },
      "source": [
        "tfidf_vect = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "\n",
        "# Fit 후 dtm을 만듭니다.(문서, 단어마다 tf-idf 값을 계산합니다)\n",
        "dtm_tfidf_news = tfidf_vect.fit_transform(df['text'])\n",
        "\n",
        "dtm_tfidf_news = pd.DataFrame(dtm_tfidf_news.todense(), columns=tfidf_vect.get_feature_names())\n",
        "dtm_tfidf_news"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>2016</th>\n",
              "      <th>according</th>\n",
              "      <th>administration</th>\n",
              "      <th>america</th>\n",
              "      <th>american</th>\n",
              "      <th>americans</th>\n",
              "      <th>asked</th>\n",
              "      <th>called</th>\n",
              "      <th>campaign</th>\n",
              "      <th>china</th>\n",
              "      <th>city</th>\n",
              "      <th>clinton</th>\n",
              "      <th>committee</th>\n",
              "      <th>congress</th>\n",
              "      <th>country</th>\n",
              "      <th>court</th>\n",
              "      <th>day</th>\n",
              "      <th>democratic</th>\n",
              "      <th>democrats</th>\n",
              "      <th>department</th>\n",
              "      <th>did</th>\n",
              "      <th>don</th>\n",
              "      <th>donald</th>\n",
              "      <th>election</th>\n",
              "      <th>federal</th>\n",
              "      <th>foreign</th>\n",
              "      <th>friday</th>\n",
              "      <th>going</th>\n",
              "      <th>government</th>\n",
              "      <th>group</th>\n",
              "      <th>hillary</th>\n",
              "      <th>house</th>\n",
              "      <th>image</th>\n",
              "      <th>including</th>\n",
              "      <th>just</th>\n",
              "      <th>know</th>\n",
              "      <th>law</th>\n",
              "      <th>like</th>\n",
              "      <th>make</th>\n",
              "      <th>...</th>\n",
              "      <th>public</th>\n",
              "      <th>republican</th>\n",
              "      <th>republicans</th>\n",
              "      <th>reuters</th>\n",
              "      <th>right</th>\n",
              "      <th>russia</th>\n",
              "      <th>russian</th>\n",
              "      <th>said</th>\n",
              "      <th>say</th>\n",
              "      <th>saying</th>\n",
              "      <th>says</th>\n",
              "      <th>secretary</th>\n",
              "      <th>security</th>\n",
              "      <th>senate</th>\n",
              "      <th>state</th>\n",
              "      <th>statement</th>\n",
              "      <th>states</th>\n",
              "      <th>support</th>\n",
              "      <th>tax</th>\n",
              "      <th>think</th>\n",
              "      <th>thursday</th>\n",
              "      <th>time</th>\n",
              "      <th>told</th>\n",
              "      <th>trump</th>\n",
              "      <th>tuesday</th>\n",
              "      <th>twitter</th>\n",
              "      <th>united</th>\n",
              "      <th>vote</th>\n",
              "      <th>want</th>\n",
              "      <th>war</th>\n",
              "      <th>washington</th>\n",
              "      <th>way</th>\n",
              "      <th>wednesday</th>\n",
              "      <th>week</th>\n",
              "      <th>white</th>\n",
              "      <th>women</th>\n",
              "      <th>work</th>\n",
              "      <th>world</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100575</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045557</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110597</td>\n",
              "      <td>0.042812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049596</td>\n",
              "      <td>0.284336</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042666</td>\n",
              "      <td>0.0477</td>\n",
              "      <td>0.034209</td>\n",
              "      <td>0.045191</td>\n",
              "      <td>0.160846</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093176</td>\n",
              "      <td>0.081828</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049987</td>\n",
              "      <td>0.255902</td>\n",
              "      <td>0.425183</td>\n",
              "      <td>0.031260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.190021</td>\n",
              "      <td>0.044146</td>\n",
              "      <td>0.095490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.167187</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040103</td>\n",
              "      <td>0.048030</td>\n",
              "      <td>0.538793</td>\n",
              "      <td>0.048767</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037807</td>\n",
              "      <td>0.216284</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050922</td>\n",
              "      <td>0.041953</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100209</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083214</td>\n",
              "      <td>0.046387</td>\n",
              "      <td>0.101548</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043996</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051551</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.131273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.539628</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049684</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048887</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045941</td>\n",
              "      <td>0.243594</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.236661</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.036709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230136</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055825</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043904</td>\n",
              "      <td>0.054278</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053640</td>\n",
              "      <td>0.045768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033545</td>\n",
              "      <td>0.049645</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.280375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058103</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.084082</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.331559</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.054643</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133944</td>\n",
              "      <td>0.049777</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049247</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063567</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.120632</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.324988</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060814</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.132262</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.216814</td>\n",
              "      <td>0.064308</td>\n",
              "      <td>0.063207</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055879</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060578</td>\n",
              "      <td>0.098338</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.153468</td>\n",
              "      <td>0.063747</td>\n",
              "      <td>0.037494</td>\n",
              "      <td>0.110980</td>\n",
              "      <td>0.562291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.227917</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133686</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058493</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.046991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.518833</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.120193</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099809</td>\n",
              "      <td>0.111275</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055045</td>\n",
              "      <td>0.052769</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046789</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.177079</td>\n",
              "      <td>0.076849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.077351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.377219</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.084706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.165715</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071279</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.057150</td>\n",
              "      <td>0.075498</td>\n",
              "      <td>0.179143</td>\n",
              "      <td>0.176077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.084377</td>\n",
              "      <td>0.205456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154605</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071253</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.293697</td>\n",
              "      <td>0.103886</td>\n",
              "      <td>0.119046</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079765</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063161</td>\n",
              "      <td>0.516187</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.069510</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220501</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065170</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064341</td>\n",
              "      <td>0.279229</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056210</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051976</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059163</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.041531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065091</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.126316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056175</td>\n",
              "      <td>0.046171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059560</td>\n",
              "      <td>0.048287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037951</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.201857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.163529</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.262577</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.061821</td>\n",
              "      <td>0.152798</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.151537</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053412</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060671</td>\n",
              "      <td>0.284153</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44893</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.140925</td>\n",
              "      <td>0.244637</td>\n",
              "      <td>0.267438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.123116</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.202257</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.246040</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063161</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.136409</td>\n",
              "      <td>0.143978</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.205959</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.213275</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.104177</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.223114</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110637</td>\n",
              "      <td>0.123346</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.122034</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.265776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44894</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.251346</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.417148</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.262100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.239829</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.215802</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.277695</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.224349</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44895</th>\n",
              "      <td>0.059549</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.051426</td>\n",
              "      <td>0.056219</td>\n",
              "      <td>0.055107</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030537</td>\n",
              "      <td>0.028740</td>\n",
              "      <td>0.050931</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032356</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032586</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047862</td>\n",
              "      <td>0.031722</td>\n",
              "      <td>0.054480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030819</td>\n",
              "      <td>0.047699</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.274439</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051729</td>\n",
              "      <td>0.106292</td>\n",
              "      <td>0.054319</td>\n",
              "      <td>0.027423</td>\n",
              "      <td>0.222328</td>\n",
              "      <td>0.074027</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083824</td>\n",
              "      <td>0.023841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.103441</td>\n",
              "      <td>0.131024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.148060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027495</td>\n",
              "      <td>0.031151</td>\n",
              "      <td>0.324714</td>\n",
              "      <td>0.027380</td>\n",
              "      <td>0.134500</td>\n",
              "      <td>0.026847</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027260</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.131397</td>\n",
              "      <td>0.063399</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028464</td>\n",
              "      <td>0.046902</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.112029</td>\n",
              "      <td>0.095259</td>\n",
              "      <td>0.441888</td>\n",
              "      <td>0.129645</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.128265</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102756</td>\n",
              "      <td>0.144078</td>\n",
              "      <td>0.111739</td>\n",
              "      <td>0.021805</td>\n",
              "      <td>0.146757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44896</th>\n",
              "      <td>0.110016</td>\n",
              "      <td>0.109462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.407242</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.112835</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113877</td>\n",
              "      <td>0.176245</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.235649</td>\n",
              "      <td>0.100355</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.091180</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079988</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.080918</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105174</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.095807</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106473</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.080570</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44897</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.373877</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.225790</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074055</td>\n",
              "      <td>0.073408</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.069594</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.089626</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.146921</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.082222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066637</td>\n",
              "      <td>0.075788</td>\n",
              "      <td>0.150433</td>\n",
              "      <td>0.061822</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064655</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.081256</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.075204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038611</td>\n",
              "      <td>0.071762</td>\n",
              "      <td>0.155225</td>\n",
              "      <td>0.083389</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125906</td>\n",
              "      <td>0.079622</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.191056</td>\n",
              "      <td>0.061457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184681</td>\n",
              "      <td>0.067634</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.071517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063412</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44898 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            000      2016  according  ...     world      year     years\n",
              "0      0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.131273\n",
              "1      0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
              "2      0.000000  0.063567   0.000000  ...  0.000000  0.046789  0.000000\n",
              "3      0.000000  0.177079   0.076849  ...  0.000000  0.065170  0.000000\n",
              "4      0.000000  0.064341   0.279229  ...  0.060671  0.284153  0.000000\n",
              "...         ...       ...        ...  ...       ...       ...       ...\n",
              "44893  0.000000  0.140925   0.244637  ...  0.265776  0.000000  0.000000\n",
              "44894  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000\n",
              "44895  0.059549  0.029624   0.051426  ...  0.111739  0.021805  0.146757\n",
              "44896  0.110016  0.109462   0.000000  ...  0.000000  0.080570  0.000000\n",
              "44897  0.000000  0.000000   0.373877  ...  0.000000  0.063412  0.000000\n",
              "\n",
              "[44898 rows x 100 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DupYYFkjOSq",
        "outputId": "590dd906-eba0-4350-c8fa-026c84cd58c1"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# dtm을 사용히 NN 모델을 학습시킵니다. (디폴트)최근접 5 이웃.\n",
        "nn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
        "nn.fit(dtm_tfidf_news)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfdHePcbjQof",
        "outputId": "21606c24-5c54-4310-866d-5bd117497767"
      },
      "source": [
        "nn.kneighbors([dtm_tfidf_news.iloc[42]])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.38516929, 0.46109983, 0.57760059, 0.65143189]]),\n",
              " array([[  42,   23,    1,  214, 1463]]))"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ6i0nXZROJL",
        "outputId": "732ea269-3417-426d-8373-e6c7e94c4e65"
      },
      "source": [
        "for idx in nn.kneighbors([dtm_tfidf_news.iloc[42]])[1][0]:\n",
        "  print('idx :', idx, 'label :', df.label[idx])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "idx : 42 label : 0\n",
            "idx : 23 label : 0\n",
            "idx : 1 label : 0\n",
            "idx : 214 label : 0\n",
            "idx : 1463 label : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghQSPbQE7P8b"
      },
      "source": [
        "## 2.2 Keras Embedding을 사용하여 분류하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEl8iU6j8I4M"
      },
      "source": [
        "### 2.2.0 데이터셋 split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJYxNZGuUBEI"
      },
      "source": [
        "- Train, Test 데이터셋으로 분리(Split)하여 주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55XSZyY_Sj54"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "fake = pd.read_csv('/content/drive/MyDrive/Fake.csv')\n",
        "true = pd.read_csv('/content/drive/MyDrive/True.csv')\n",
        "\n",
        "true['label'] = [0 for _ in range(true.shape[0])]\n",
        "fake['label'] = [0 for _ in range(fake.shape[0])]\n",
        "\n",
        "df = pd.concat([true, fake])\n",
        "\n",
        "true.shape, fake.shape, df.shape\n",
        "\n",
        "train, test = train_test_split(df[['title', 'label']], train_size=0.85, test_size=0.15, \n",
        "                              stratify=df['label'], random_state=2)\n",
        "\n",
        "X_train, y_train, X_test, y_test = train.drop('label', axis = 1), train.label, test.drop('label', axis = 1), test.label"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1xXxSMn7fyt"
      },
      "source": [
        "### 2.2.1 단어 벡터의 평균을 이용하여 분류해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_ZsjAVg7ndG"
      },
      "source": [
        "N422에서 했던 단어 임베딩 벡터의 평균을 사용하여 문장을 분류하는 작업을 수행해봅시다.<br/>\n",
        "인스턴스마다 텍스트 길이가 길고 시간이 오래 걸리므로 시간상 epoch 수를 **10 이하**로 하는 것을 추천드립니다.<br/>\n",
        "모델 구동이 목적이므로 임베딩 차원 수를 크지 않게(50이하)로 설정해주세요.<br/>\n",
        "**권장사항 : `max_len` 은 텍스트 길이 평균보다 높게 설정해주세요.**<br/>\n",
        "\n",
        "> **Tip : 모델이 학습하는 동안 2.2.3의 내용을 작성하면 시간을 절약할 수 있습니다.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLLTJCRLMPOS",
        "outputId": "723ce538-9c33-421c-f968-92d3ee4e1768"
      },
      "source": [
        "maxlen = np.mean([len(sent) for sent in X_train.title], dtype=int)\n",
        "\n",
        "\n",
        "# TF-IDF vectorizer. 테이블을 작게 만들기 위해 max_features=15로 제한하였습니다.\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=15)\n",
        "\n",
        "# Fit 후 dtm을 만듭니다.(문서, 단어마다 tf-idf 값을 계산합니다)\n",
        "X_train = tfidf.fit_transform(X_train.title)\n",
        "X_train = pd.DataFrame(X_train.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "X_test = tfidf.fit_transform(X_test.title)\n",
        "X_test = pd.DataFrame(X_test.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train.values, maxlen=maxlen)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test.values, maxlen=maxlen)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape, len(X_train), len(X_test)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((38163, 80), (38163,), (6735, 80), (6735,), 38163, 6735)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGrseFKvL4RK",
        "outputId": "30038647-0866-4078-d7c3-2a323da91ac7"
      },
      "source": [
        "print(maxlen)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHber3qBBwOl"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model2 = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POGI4q2PmTLN",
        "outputId": "00c8137f-bb5b-404c-bc98-3ff3d893c1e3"
      },
      "source": [
        "model2.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model2.fit(\n",
        "    X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test)\n",
        ")"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1193/1193 [==============================] - 48s 40ms/step - loss: 0.0036 - accuracy: 0.9996 - val_loss: 1.1921e-07 - val_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1193/1193 [==============================] - 47s 40ms/step - loss: 2.9832e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SzUwLkcAK1A"
      },
      "source": [
        "### 2.2.2 LSTM을 사용하여 텍스트 분류 수행해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4UZ9ZqOAIjw"
      },
      "source": [
        "N423에서 했던 단어 임베딩 벡터의 평균을 사용하여 문장을 분류하는 작업을 수행해봅시다.<br/>\n",
        "인스턴스마다 텍스트 길이가 길어 시간이 매우 오래 걸리므로 <br/>\n",
        "**층을 최소한으로 쌓고**, epoch 수를 **3 이하**로 하는 것을 추천드립니다.<br/>\n",
        "\n",
        "> **Tip : 모델이 학습하는 동안 2.2.3의 내용을 작성하면 시간을 절약할 수 있습니다.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PLNlzEVBSyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af40dc1-5f49-4726-b82e-11143debf2ab"
      },
      "source": [
        "fake = pd.read_csv('/content/drive/MyDrive/Fake.csv')\n",
        "true = pd.read_csv('/content/drive/MyDrive/True.csv')\n",
        "\n",
        "true['label'] = [0 for _ in range(true.shape[0])]\n",
        "fake['label'] = [0 for _ in range(fake.shape[0])]\n",
        "\n",
        "df = pd.concat([true, fake])\n",
        "\n",
        "true.shape, fake.shape, df.shape\n",
        "\n",
        "train, test = train_test_split(df[['title', 'label']], train_size=0.85, test_size=0.15, \n",
        "                              stratify=df['label'], random_state=2)\n",
        "\n",
        "X_train, y_train, X_test, y_test = train.drop('label', axis = 1), train.label, test.drop('label', axis = 1), test.label\n",
        "\n",
        "maxlen = np.mean([len(sent) for sent in X_train.title], dtype=int)\n",
        "\n",
        "\n",
        "# TF-IDF vectorizer. 테이블을 작게 만들기 위해 max_features=15로 제한하였습니다.\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=15)\n",
        "\n",
        "# Fit 후 dtm을 만듭니다.(문서, 단어마다 tf-idf 값을 계산합니다)\n",
        "X_train = tfidf.fit_transform(X_train.title)\n",
        "X_train = pd.DataFrame(X_train.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "X_test = tfidf.fit_transform(X_test.title)\n",
        "X_test = pd.DataFrame(X_test.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train.values, maxlen=maxlen)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test.values, maxlen=maxlen)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape, len(X_train), len(X_test)\n",
        "\n",
        "# 파라미터를 설정합니다.\n",
        "max_features = 20000\n",
        "maxlen = maxlen\n",
        "batch_size = 32\n",
        "\n",
        "# model을 정의합니다.\n",
        "\"\"\"\n",
        "keras의 기본 Embedding 벡터를 사용하였으며\n",
        "LSTM 층에 dropout/recurrent_dropout을 적용하였습니다.\n",
        "\"\"\"\n",
        "model3 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(max_features, 128),\n",
        "  tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model3.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model3.summary()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1W3zVPBsCDR",
        "outputId": "58186830-f013-4af2-e39f-43d94cb1b013"
      },
      "source": [
        "unicorns = model3.fit(X_train, y_train,\n",
        "          batch_size=batch_size, \n",
        "          epochs=2, \n",
        "          validation_data=(X_test,y_test))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1193/1193 [==============================] - 375s 282ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 2.7185e-06 - val_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "1193/1193 [==============================] - 341s 286ms/step - loss: 1.8011e-06 - accuracy: 1.0000 - val_loss: 1.1111e-06 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tQz12auys4xt",
        "outputId": "e594f058-43b8-4c4d-d6a0-b15d6d932e68"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsNOs2wmBV1z"
      },
      "source": [
        "### 2.2.3 위에서 실행한 내용에 대해 다시 알아봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMR2t2wOEPYm"
      },
      "source": [
        "#### a) 데이터셋을 학습할 때 사용하는 `pad_sequences`  메서드에 대해 설명해주세요.<br/>어떤 기능을 하나요? 모델을 학습할 때 왜 필요한가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_ECWSYQ4JV"
      },
      "source": [
        "가변 시퀀스들을 채우기 위해서 사용하며, 부자연스러운 시퀀스 문제를 해결하기 위해서 사용한다. \n",
        "기계는 전부 동일한 길이의 문서에 대해서는 하나의 행렬로 보고 묶어서 연산 할 수 있기 때문에 pad_sequences로 길이를 일정하게 맞춰서 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeZ6AcloEYD5"
      },
      "source": [
        "#### b) 2.2.1과 2.2.2에서 사용한 각 모델의 evaluation 성능은 어떻게 나왔나요?<br/>각 모델의 장단점은 무엇이라고 생각하나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SyqQzzHQ4EP"
      },
      "source": [
        "TF-IDF는 단기 의존성 문제를 가지지만 LSTM은 단기의존성 문제를 해결하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUwKojAGM82f"
      },
      "source": [
        "#### c) 종래의 RNN(Recurrent Neural Networks) 대신 LSTM(Long-Short Term Memory)을 사용하는 이유는 무엇인가요?<br/>(i.e. RNN에 비해 LSTM의 좋은 점을 설명해주세요.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y44XfugQ_HR"
      },
      "source": [
        "RNN은 앞쪽 시퀀스의 데이터가 거의 지워져 단기 의존성 문제가 있는데 이를 해결하기위해 LSTM을 사용 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J1kzTmDEaLU"
      },
      "source": [
        "#### d) LSTM이나 RNN을 사용하는 예시를 **3개**이상 제시하고 해당되는 경우에 왜 LSTM이나 RNN을 사용하는 것 적절한지 간단하게 설명해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck8GPjc_Q_vA"
      },
      "source": [
        "이미지 켑셔닝, 감성 분석, 기계번역, 비디오 프레임별로 분류 하는데 사용되며 시퀀셜한 데이터들을 읽기에 LSTM과 RNN이 좋아서 사용 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4pP7g8DE0Cz"
      },
      "source": [
        "#### e) 이외에 N424 에서 배운 자연어처리 모델과 관련된 키워드를 3개 이상 적어주세요. <br/> (해당 키워드에 대한 설명은 옵션입니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-sO4mMuRAhp"
      },
      "source": [
        "Transformer : 기계번역을 위한 새로운 모델로 등장한 Attention 매커니즘을 극대화하여 뛰어난 번역성능을 가진 모델로 RNN을 사용하지 않아 병렬화가 되서 빠르고 효과가 좋다. GPU 연산에 최적화 되어 있다.\n",
        "\n",
        "GPT : 트렌스포머의 디코더 기반 모델이다. 비지도 학습이며 다양하고 엄청난 양의 데이터를 통해서 프리 트레이닝 된다.\n",
        "\n",
        "BERT : 양쪽 문맥을 동시에 탐색한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRn44qjhUo9j"
      },
      "source": [
        "# Advanced Goals: 3점을 획득하기 위해선 아래의 조건 중 하나 이상을 만족해야합니다\n",
        " \n",
        "- 2.1 에서 TF-IDF(`TfidfVectorizer`)가 아닌 방법을 사용하여 유사도 검색을 수행해보세요.<br/>\n",
        "TF-IDF와 해당 방법의 차이를 설명해주세요. \n",
        "- 2.2 에서 사용한 방법을 재사용하되 하이퍼 파라미터를 조정하거나 모델 구조를 변경하여 성능을 올려봅시다.<br/>**(주의 : GridSearch, RandomSearch 등의 방법을 사용하여도 좋으나 시간이 오래 걸리므로 범위를 잘 선택해야 합니다.)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz2uaXFYUo9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03def62-ddbf-4826-fc50-64ab7b737e6f"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "# 코드 실행 전 seed를 지정하겠습니다.\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "fake = pd.read_csv('/content/drive/MyDrive/Fake.csv')\n",
        "true = pd.read_csv('/content/drive/MyDrive/True.csv')\n",
        "\n",
        "true['label'] = [0 for _ in range(true.shape[0])]\n",
        "fake['label'] = [0 for _ in range(fake.shape[0])]\n",
        "\n",
        "df = pd.concat([true, fake], ignore_index=True)\n",
        "\n",
        "true.shape, fake.shape, df.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21417, 5), (23481, 5), (44898, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1x8MwU9fKIS"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "def tokenize(document):\n",
        "    doc = nlp(document)\n",
        "    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True) and (token.is_alpha == True)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1goDNAFmiYA5",
        "outputId": "b2c1b2d2-a957-4486-ffcc-32622f9b95c6"
      },
      "source": [
        "vectorize = HashingVectorizer(\n",
        "    tokenizer=tokenize,\n",
        "    n_features=7               # 기본 feature 수를 설정하며 기본값이 2의 20승이다.\n",
        ")\n",
        "X = vectorize.fit_transform(df.title.iloc[0:100])\n",
        "\n",
        "print(X.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(100, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoXhwEcXdKGt",
        "outputId": "ae02a741-e6b0-4a1b-e648-5ad4ebb103ef"
      },
      "source": [
        "# search 문장 벡터\n",
        "srch_vector = vectorize.transform([\n",
        "    df.title[42]\n",
        "])\n",
        "\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        " \n",
        "# linear_kernel는 두 벡터의 dot product 이다.\n",
        "cosine_similar = linear_kernel(srch_vector, X).flatten()\n",
        "# cosine_similar = (srch_vector*X.T).toarray().flatten()\n",
        " \n",
        "# 유사한 df.title index\n",
        "sim_rank_idx = cosine_similar.argsort()[::-1]\n",
        "print(sim_rank_idx[0:5])\n",
        "#[2 3 4 1 0]\n",
        " \n",
        "for i in sim_rank_idx[0:5]:\n",
        "    if cosine_similar[i] > 0:\n",
        "        print('{} / score : {}'.format(df.title[i], cosine_similar[i]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[42 23  6 77 47]\n",
            "U.S. court rejects Trump bid to stop transgender military recruits on Jan. 1 / score : 1.0\n",
            "Second court rejects Trump bid to stop transgender military recruits / score : 0.9198662110078\n",
            "Trump says Russia probe will be fair, but timeline unclear: NYT / score : 0.8864052604279183\n",
            "Trump's Supreme Court appointee Gorsuch plots rightward course / score : 0.8090398349558905\n",
            "U.S. launches effort to reduce reliance on imports or critical minerals / score : 0.8090398349558905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4uvWEmDegNA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}