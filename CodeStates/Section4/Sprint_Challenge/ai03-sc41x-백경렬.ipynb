{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ai-sc41x.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "nteract": {
      "version": "0.23.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVNDxiWkw5y7"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## AI SC41x\n",
        "\n",
        "---\n",
        "# Sprint Challenge - 신경망(Neural Network) 기본기 다지기\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. 신경망 정의하기 \n",
        "2. 퍼셉트론 정의\n",
        "    - 퍼셉트론(Perceptron)\n",
        "    - 다층 퍼셉트론(Multilayer Perceptron)\n",
        "    - 분석과 비교\n",
        "4. Keras 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9qf0cRbw5y8"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. 신경망 용어 정의\n",
        "### 1.1 아래에 주어진 신경망 개념에 사용 되는 용어들을 자신만의 언어로 정의해보세요\n",
        "꼭 \"자신의 언어\"로 써보시고, 정리해보고, 요약해보세요 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfBaZTgw0PhB"
      },
      "source": [
        "- **Neuron: 신경계를 구성하는 세포 **\n",
        "- **Input Layer: 데이터셋으로부터 입력을 받고, 보통 어떤 계산도 수행하지 않고, 그냥 값을 전달하기만 하는 역할을 한다. **\n",
        "- **Hidden Layer: 계산이 일어나는 층이며, 입력층과 마지막 출력층 사이에 있는 층들을 은닉층이라 말한다. **\n",
        "- **Output Layer: 신경망의 가장 마지막 층이며, 활성함수가 존재한다. **\n",
        "- **Activation Function: 입력된 데이터의 가중 합을 출력 신호로 변환하는 함수이다. **\n",
        "- **Back Propagation: 원하는 값과 예측하는 값의 오차에 따라 각 노드에 따른 가중치를 편미분한 값을 바탕으로 수정하여 결과값이 원하는 값을 도출하도록 하는 과정이다. **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdcS-Jx2w5y8"
      },
      "source": [
        "### 1.2 역전파 설명해보기\n",
        "이번에는 역전파 (Back Propagation)를 조금 더 디테일하게 설명해보겠습니다. \n",
        "<br> <b>초등학생</b>에게 설명한다는 생각으로 단어들을 선정해 주세요. <i>외부 자료나 이미지를 설명에 활용하셔도 좋습니다</i>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s43PkOEO2nGY"
      },
      "source": [
        "씻으려고 물 온도를 조절할 때 뜨거우면 찬물 쪽을 더 틀고, 아직도 뜨겁다 싶으면 찬물을 조금 더 틀고, 너무 차가워졌다 싶으면 다시 뜨거운 물을 더 틀죠? 이렇게 조금 조금씩 현재 상황에 맞춰서 원하는 온도로 맞춰 가는 과정을 역전파 라고 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7ZSjf9Uw5y8"
      },
      "source": [
        "### 1.3 퍼셉트론 정의해보기\n",
        "신경망 수업 첫날에 들었었던 퍼셉트론의 간단한 개념으로 예측하는 과정을 설명해보세요. <br> <b>입력</b>에서 <b>출력</b>으로 어떻게 변경되는 지 단계별로 설명해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNKngzxT39QY"
      },
      "source": [
        "다수의 입력을 받아서 입력 받은 노드들에게 가중치를 두어 계산하여 하나의 출력값을 얻는 과정이다.\n",
        "1. 입력을 받는다.\n",
        "2. 입력 받은 각각의 데이터에 가중치를 준다.\n",
        "3. 가중치를 받은 데이터들을 계산해서 하나의 값을 얻는다.\n",
        "4. 얻은 하나의 값을 출력한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A2h2zpCw5y8"
      },
      "source": [
        "<a id=\"Q2\"></a>\n",
        "## 2. 단순 퍼셉트론\n",
        "\n",
        "이번에는 TensorFlow, keras를 사용하여 두 개의 신경망을 직접 구축한 뒤,\n",
        "<br> 아래 임의로 제공 된 $X, y$를 이용하여 두 모델에 적용한 뒤 결과를 비교해보세요. \n",
        "먼저 사용할 데이터는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "TWdXFK9Yw5y8",
        "outputId": "48560e5b-e733-47d8-9ece-8dd82938ad0c"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
        "                     np.linspace(-3, 3, 50))\n",
        "rng = np.random.RandomState(0)\n",
        "\n",
        "\n",
        "\"model1과 model2를 구축할 때 아래의 X & y를 사용하세요\"\n",
        "X = rng.randn(300, 2)\n",
        "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n",
        "             dtype=int)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], s=y)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f90e1cffe50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjr4UoQXw5y8"
      },
      "source": [
        "### 단순 퍼셉트론 구현\n",
        "Keras로 <b>sigmoid activation function</b>을 포함한 dense layer 1개가 있는</b> `model1`을 만들어 학습시키고 `h1`에 저장하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRXT1mpV5UpE",
        "outputId": "bb3ef103-ead7-47ba-a4c9-bccdf732d0d0"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((300, 2), (300,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKZXIC1ew5y8",
        "outputId": "3dae54df-79f4-477f-9426-6b97d9381cde"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model1 = Sequential([\n",
        "    Dense(64, activation='relu')\n",
        "])\n",
        "\n",
        "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "h1 = model1.fit(X, y, epochs = 30)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 10.5784 - acc: 0.2300\n",
            "Epoch 2/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 10.1770 - acc: 0.2433\n",
            "Epoch 3/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 9.8895 - acc: 0.2467\n",
            "Epoch 4/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 9.5445 - acc: 0.2467\n",
            "Epoch 5/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 9.1820 - acc: 0.2500\n",
            "Epoch 6/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.8989 - acc: 0.2800\n",
            "Epoch 7/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.5871 - acc: 0.3000\n",
            "Epoch 8/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 8.2496 - acc: 0.3267\n",
            "Epoch 9/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.9185 - acc: 0.3500\n",
            "Epoch 10/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.5713 - acc: 0.3800\n",
            "Epoch 11/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 7.2569 - acc: 0.4133\n",
            "Epoch 12/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.9543 - acc: 0.4233\n",
            "Epoch 13/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.6089 - acc: 0.4567\n",
            "Epoch 14/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 6.1444 - acc: 0.4900\n",
            "Epoch 15/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 5.5815 - acc: 0.5367\n",
            "Epoch 16/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 5.0576 - acc: 0.6067\n",
            "Epoch 17/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 4.6850 - acc: 0.6233\n",
            "Epoch 18/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 3.8343 - acc: 0.6800\n",
            "Epoch 19/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 3.4163 - acc: 0.6967\n",
            "Epoch 20/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.8220 - acc: 0.7233\n",
            "Epoch 21/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.5317 - acc: 0.7433\n",
            "Epoch 22/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.3455 - acc: 0.7433\n",
            "Epoch 23/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.2893 - acc: 0.7433\n",
            "Epoch 24/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.2837 - acc: 0.7467\n",
            "Epoch 25/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.3185 - acc: 0.7367\n",
            "Epoch 26/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.3302 - acc: 0.7333\n",
            "Epoch 27/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 2.3360 - acc: 0.7400\n",
            "Epoch 28/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.2972 - acc: 0.7400\n",
            "Epoch 29/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.2909 - acc: 0.7367\n",
            "Epoch 30/30\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 2.2883 - acc: 0.7367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ugkziwew5y8"
      },
      "source": [
        "\n",
        "### Multi-Layer Perceptron (MLP)\n",
        "이번에는 여러층의 레이어들을 쌓은 MLP 모델을 만들어보겠습니다. 아래는 간략한 가이드입니다 :\n",
        "- 2개의 은닉층 (출력 수를 맞추는 것까지 3개의 Dense를 사용할 것은 추천)\n",
        "- 노드의 개수는 8-32개 내에서 변경해서 사용해보세요.\n",
        "- Activation function과 optimizer는 이번 주에 배운 것들 중에서 자유롭게 골라보세요.\n",
        "- 아래 만들어진 Callback function을 모델에 통합해서 사용하세요\n",
        "\n",
        "MLP로 만들어진 `model2` 를 만들어 학습하고 `h2`에 저장하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BoIn7iBw5y8"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback): \n",
        "    def on_epoch_end(self, epoch, logs={}): \n",
        "        if(logs.get('acc') > .90):   \n",
        "            self.model.stop_training = True"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B63dciqEw5y8",
        "outputId": "6ced7769-3adc-4cd5-a886-1b8d7b13d1ac"
      },
      "source": [
        "model2 = Sequential([\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'), \n",
        "    Dense(2, activation='softmax') \n",
        "])\n",
        "\n",
        "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        " \n",
        "h2 = model2.fit(X,y, batch_size=40, epochs=100, validation_data=(X,y), callbacks=[myCallback()])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 22ms/step - loss: 0.6922 - acc: 0.5233 - val_loss: 0.6662 - val_acc: 0.6200\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6510 - acc: 0.7033 - val_loss: 0.6281 - val_acc: 0.7767\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6146 - acc: 0.8000 - val_loss: 0.5937 - val_acc: 0.8200\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5823 - acc: 0.8333 - val_loss: 0.5649 - val_acc: 0.8700\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5545 - acc: 0.8833 - val_loss: 0.5381 - val_acc: 0.9033\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5278 - acc: 0.9067 - val_loss: 0.5129 - val_acc: 0.9167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn2TsE76w5y8"
      },
      "source": [
        "### Analyze and Compare\n",
        "\n",
        "코드를 시작하기 전에 추가 라이브러리를 설치해야 합니다. 스프린트 과제에 사용 중인 환경에 패키지 `mlxtend`를 설치합니다. 설치코드를 직접 제작해보세요. 기존 자료들을 참고하면 쉽게 설치할 수 있을 것입니다. \n",
        "\n",
        "아래 셀은 모형의 의사결정 경계도(\"model1\" 및 \"model2\")를 생성합니다. 그림을 검토합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96hsbGktVlgL",
        "outputId": "4ac216f0-0aeb-4404-d93c-b8fe8fd3b91b"
      },
      "source": [
        "# mlxtend 라이브러리를 설치합니다\n",
        "!pip install mlxtend"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (57.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.5.1->mlxtend) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.1->mlxtend) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "ox13rgjTw5y8",
        "outputId": "379291a1-ae7d-4b27-b25f-2d0987d1f6ad"
      },
      "source": [
        "# 이 셀의 코드는 변경하지 마세요\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - .2, X[:, 0].max() + .2\n",
        "y_min, y_max = X[:, 1].min() - .2, X[:, 1].max() + .2\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "\n",
        "for clf, hist, name, grd in zip([model1,model2], [h1, h2],['Perceptron', 'Multi-Layer Perceptron'],[1,2]):\n",
        "\n",
        "    ax = plt.subplot(1,2, grd)\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
        "    title = f\"{name} with {hist.history['accuracy'][-1]:,.2f} Accuracy\"\n",
        "    plt.title(title)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-f0aad60f5624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_decision_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{name} with {hist.history['accuracy'][-1]:,.2f} Accuracy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py\u001b[0m in \u001b[0;36mplot_decision_regions\u001b[0;34m(X, y, clf, feature_index, filler_feature_values, filler_feature_ranges, ax, X_highlight, res, legend, hide_spines, markers, colors, scatter_kwargs, contourf_kwargs, scatter_highlight_kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mX_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiller_feature_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;31m# Plot decisoin region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# Make sure contourf_kwargs has backwards compatible defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 23887872 into shape (432,864)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qBWszwMw5y8"
      },
      "source": [
        "#### 대부분의 Perceptron(\"model1\")의 정확도가 50-70% 정도로 나오실 것인데요, 왜 그것밖에 되지 않을까요? <br> 데이터 X와 레이블 y의 관계를 보다 정확하게 학습할 수 있는 다층 퍼셉트론의 구조적인 특징은 무엇일까요? (설명을 하실 때 우리가 배웠었던 특징 추출의 관점에서 설명을 해보시길 바랍니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kDAzIswEeR-"
      },
      "source": [
        "model1 에서는 입력층과 출력층만 존재하여 선형적인 예측만 가능하지만,\n",
        "\n",
        "model2 에서는 다층 퍼셉트론을 이용하여 중간에 히든레이어를 넣어 비선형적인 예측이 가능하게 되어 보다 더 정확한 학습을 할 수 있게 된다.\n",
        "\n",
        "특징 추출의 관점에서 설명하자면, \n",
        "\n",
        "다층 퍼셉트론에서는 입력 데이터에서 특징을 추출하여 새로운 입력 데이터를 만들어 다시 학습을 시킨다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEPmr-_w5y8"
      },
      "source": [
        "## 3. Keras MLP 튜닝\n",
        "\n",
        "이번에는 Keras 라이브러리를 사용하여 선택한 MultiLayer Perceptron(MLP) 아키텍처를 구현해보겠습니다. 극히 단순한 모델부터 복잡한 모델까지 만들어 보실 수 있습니다. 모델을 학습하고 정확도를 구해보세요. \n",
        "<br> 그런 다음 최소 <b>두 개</b>의 파라미터를 튜닝한 후에 다시 모형의 정확도를 구해보고 이전 모델과 비교해보세요. \n",
        "<br> 아래 Cell에서 심장병 데이터 세트를 불러 온 후 이진 분류 모델 (binary classification model) 을 만들어 보세요. 이진 분류 작업에 적절한 손실 함수를 사용하고, 신경망의 마지막 계층에서 적절한 출력값과 활성화 함수를 사용합니다. \n",
        "<br> 세부적인 출력을 사용하여 빠르게 수렴할 수 있도록 모델을 학습해보는 것도 중요합니다. GridSearchCV 또는 RandomSearchCV를 사용하여 모델을 하이퍼 파라미터들을 튜닝해봅니다. (최소 두 개의 하이퍼 파라미터를 튜닝해봅니다) 하이퍼 파라미터 튜닝 시 새로운 각 실험에 대해 코드 셀을 추가하여 작업하는 방법을 배워보았는데, 그대로 이용하지 말고 변형을 해서 보여주고 싶은 내용을 정리해서 보여주세요.  \n",
        "<br> 테스트할 때 하이퍼 파라미터의 각 조합에 대한 정확도를 보고하여 가장 높은 정확도를 얻을 수 있는 결과를 쉽게 확인할 수 있도록 합니다. \n",
        "<br> 이 SC에서 **3점**을 얻으려면 **최소 3개의 파라미터**를 조정해야 합니다.\n",
        "\n",
        "- BatchNormalization을 레이어에 추가하면 성능이 많이 오를 수 있습니다. \n",
        "- 하지만 BatchNormalization을 사용하기 위해서는 Batch_size 옵션을 추가해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "F35sgQ7Cw5y8",
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "outputId": "9b6a1d4f-ed5e-44e3-e226-3a752ad435a2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/heart.csv')\n",
        "df = df.sample(frac=1)\n",
        "print(df.shape)\n",
        "df.head()\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>61</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>166</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>124</td>\n",
              "      <td>209</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>132</td>\n",
              "      <td>247</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>143</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>120</td>\n",
              "      <td>231</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>182</td>\n",
              "      <td>1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>110</td>\n",
              "      <td>175</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>123</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "279   61    1   0       138   166    0  ...      1      3.6      1   1     2       0\n",
              "69    62    0   0       124   209    0  ...      0      0.0      2   0     2       1\n",
              "251   43    1   0       132   247    1  ...      1      0.1      1   4     3       0\n",
              "259   38    1   3       120   231    0  ...      1      3.8      1   0     3       0\n",
              "27    51    1   2       110   175    0  ...      0      0.6      2   0     2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5eoOsnfLa09",
        "outputId": "9776551c-9a05-4a5e-8804-516282bd72cf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, train_size=0.80, test_size=0.20, \n",
        "                              stratify=df['target'], random_state=2)\n",
        "\n",
        "X_train, y_train, X_test, y_test = train.drop('target', axis = 1), train.target, test.drop('target', axis = 1), test.target\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((242, 13), (242,), (61, 13), (61,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccj9k0irO7MG"
      },
      "source": [
        "단순한 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDeM_1iEw5y9",
        "outputId": "e4c523bd-42e7-42cb-9303-c0b289a51cdc"
      },
      "source": [
        "model = Sequential([\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        " \n",
        "results = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test,y_test))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 22ms/step - loss: 37.9365 - acc: 0.4545 - val_loss: 37.1152 - val_acc: 0.4590\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 35.4161 - acc: 0.4545 - val_loss: 34.6027 - val_acc: 0.4590\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 32.8613 - acc: 0.4545 - val_loss: 32.0869 - val_acc: 0.4590\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 30.3468 - acc: 0.4545 - val_loss: 29.5333 - val_acc: 0.4590\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 27.8762 - acc: 0.4545 - val_loss: 26.9573 - val_acc: 0.4590\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 25.3343 - acc: 0.4587 - val_loss: 24.4327 - val_acc: 0.4590\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 22.8663 - acc: 0.4587 - val_loss: 21.9110 - val_acc: 0.4590\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 20.4236 - acc: 0.4587 - val_loss: 19.3743 - val_acc: 0.4590\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 17.8279 - acc: 0.4587 - val_loss: 16.9750 - val_acc: 0.4426\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 15.5175 - acc: 0.4380 - val_loss: 14.6859 - val_acc: 0.4426\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13.4011 - acc: 0.3926 - val_loss: 12.7580 - val_acc: 0.3934\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11.5701 - acc: 0.3884 - val_loss: 11.1645 - val_acc: 0.3770\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10.1076 - acc: 0.3760 - val_loss: 9.8452 - val_acc: 0.4098\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 8.9427 - acc: 0.3967 - val_loss: 8.8780 - val_acc: 0.3770\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8.2090 - acc: 0.3678 - val_loss: 8.1922 - val_acc: 0.4098\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.6568 - acc: 0.3719 - val_loss: 7.8380 - val_acc: 0.4098\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.3953 - acc: 0.3636 - val_loss: 7.6407 - val_acc: 0.4262\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.2339 - acc: 0.3678 - val_loss: 7.5338 - val_acc: 0.4262\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.1977 - acc: 0.3678 - val_loss: 7.4564 - val_acc: 0.4098\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.1292 - acc: 0.3760 - val_loss: 7.4085 - val_acc: 0.4098\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.0815 - acc: 0.3802 - val_loss: 7.3668 - val_acc: 0.4098\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.0403 - acc: 0.3843 - val_loss: 7.3253 - val_acc: 0.4098\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7.0022 - acc: 0.3802 - val_loss: 7.2813 - val_acc: 0.4098\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.9555 - acc: 0.3843 - val_loss: 7.2393 - val_acc: 0.4098\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.9147 - acc: 0.3843 - val_loss: 7.1932 - val_acc: 0.4098\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.8695 - acc: 0.3884 - val_loss: 7.1497 - val_acc: 0.4098\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.8261 - acc: 0.3843 - val_loss: 7.1062 - val_acc: 0.4262\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.7775 - acc: 0.3843 - val_loss: 7.0565 - val_acc: 0.4098\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.7311 - acc: 0.3884 - val_loss: 7.0078 - val_acc: 0.4098\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.6872 - acc: 0.3926 - val_loss: 6.9613 - val_acc: 0.4098\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.6394 - acc: 0.3926 - val_loss: 6.9104 - val_acc: 0.4098\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.5912 - acc: 0.3884 - val_loss: 6.8622 - val_acc: 0.4098\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.5432 - acc: 0.3926 - val_loss: 6.8106 - val_acc: 0.4098\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.4916 - acc: 0.3884 - val_loss: 6.7627 - val_acc: 0.4262\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.4617 - acc: 0.3843 - val_loss: 6.7222 - val_acc: 0.4426\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6.3922 - acc: 0.3884 - val_loss: 6.6611 - val_acc: 0.4262\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.3489 - acc: 0.3926 - val_loss: 6.6048 - val_acc: 0.4098\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.2947 - acc: 0.3926 - val_loss: 6.5497 - val_acc: 0.4098\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6.2455 - acc: 0.3926 - val_loss: 6.4962 - val_acc: 0.4098\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.1932 - acc: 0.3926 - val_loss: 6.4435 - val_acc: 0.4098\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6.1416 - acc: 0.3926 - val_loss: 6.3915 - val_acc: 0.4098\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.0903 - acc: 0.3926 - val_loss: 6.3365 - val_acc: 0.4098\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 6.0397 - acc: 0.3967 - val_loss: 6.2814 - val_acc: 0.4098\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 5.9886 - acc: 0.3926 - val_loss: 6.2297 - val_acc: 0.4098\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9310 - acc: 0.4008 - val_loss: 6.1732 - val_acc: 0.4098\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8756 - acc: 0.4008 - val_loss: 6.1187 - val_acc: 0.4098\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8249 - acc: 0.3967 - val_loss: 6.0595 - val_acc: 0.4098\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7682 - acc: 0.3967 - val_loss: 6.0014 - val_acc: 0.4098\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7105 - acc: 0.3967 - val_loss: 5.9461 - val_acc: 0.4098\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6762 - acc: 0.3926 - val_loss: 5.8954 - val_acc: 0.4426\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6055 - acc: 0.4008 - val_loss: 5.8276 - val_acc: 0.4262\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.5410 - acc: 0.4050 - val_loss: 5.7694 - val_acc: 0.4262\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.4864 - acc: 0.4050 - val_loss: 5.7104 - val_acc: 0.4262\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.4323 - acc: 0.4050 - val_loss: 5.6521 - val_acc: 0.4262\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.3724 - acc: 0.4050 - val_loss: 5.5911 - val_acc: 0.4262\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.3164 - acc: 0.4050 - val_loss: 5.5317 - val_acc: 0.4262\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.2736 - acc: 0.3967 - val_loss: 5.4816 - val_acc: 0.4426\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.1999 - acc: 0.4050 - val_loss: 5.4191 - val_acc: 0.4426\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.1488 - acc: 0.4008 - val_loss: 5.3502 - val_acc: 0.4262\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.0814 - acc: 0.4050 - val_loss: 5.2878 - val_acc: 0.4262\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.0199 - acc: 0.4091 - val_loss: 5.2261 - val_acc: 0.4262\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4.9661 - acc: 0.4091 - val_loss: 5.1653 - val_acc: 0.4262\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4.9102 - acc: 0.4091 - val_loss: 5.1067 - val_acc: 0.4426\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4.8408 - acc: 0.4050 - val_loss: 5.0401 - val_acc: 0.4262\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4.7871 - acc: 0.4132 - val_loss: 4.9738 - val_acc: 0.4262\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4.7250 - acc: 0.4174 - val_loss: 4.9124 - val_acc: 0.4262\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 4.6657 - acc: 0.4132 - val_loss: 4.8504 - val_acc: 0.4262\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4.5999 - acc: 0.4174 - val_loss: 4.7873 - val_acc: 0.4262\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4.5428 - acc: 0.4091 - val_loss: 4.7243 - val_acc: 0.4426\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4.4799 - acc: 0.4256 - val_loss: 4.6548 - val_acc: 0.4426\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4.4151 - acc: 0.4132 - val_loss: 4.5904 - val_acc: 0.4590\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 4.3585 - acc: 0.4132 - val_loss: 4.5312 - val_acc: 0.4590\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 4.2866 - acc: 0.4050 - val_loss: 4.4658 - val_acc: 0.4590\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4.2264 - acc: 0.4050 - val_loss: 4.4002 - val_acc: 0.4590\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4.1619 - acc: 0.4174 - val_loss: 4.3305 - val_acc: 0.4590\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 4.1016 - acc: 0.4174 - val_loss: 4.2629 - val_acc: 0.4590\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4.0448 - acc: 0.4132 - val_loss: 4.2009 - val_acc: 0.4590\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.9734 - acc: 0.4174 - val_loss: 4.1336 - val_acc: 0.4590\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.9127 - acc: 0.4174 - val_loss: 4.0714 - val_acc: 0.4590\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 3.8494 - acc: 0.4132 - val_loss: 4.0029 - val_acc: 0.4590\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.7835 - acc: 0.4174 - val_loss: 3.9387 - val_acc: 0.4590\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3.7251 - acc: 0.4174 - val_loss: 3.8715 - val_acc: 0.4590\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.6587 - acc: 0.4174 - val_loss: 3.8047 - val_acc: 0.4754\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.5910 - acc: 0.4091 - val_loss: 3.7428 - val_acc: 0.4754\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.5340 - acc: 0.4132 - val_loss: 3.6746 - val_acc: 0.4754\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.4796 - acc: 0.4215 - val_loss: 3.6222 - val_acc: 0.4590\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.4174 - acc: 0.4174 - val_loss: 3.5526 - val_acc: 0.4590\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.3403 - acc: 0.4132 - val_loss: 3.4742 - val_acc: 0.4754\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.2762 - acc: 0.4339 - val_loss: 3.4065 - val_acc: 0.4754\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.2131 - acc: 0.4339 - val_loss: 3.3403 - val_acc: 0.4754\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.1518 - acc: 0.4380 - val_loss: 3.2734 - val_acc: 0.4754\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.0894 - acc: 0.4380 - val_loss: 3.2071 - val_acc: 0.4918\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 3.0203 - acc: 0.4380 - val_loss: 3.1491 - val_acc: 0.4754\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2.9584 - acc: 0.4380 - val_loss: 3.0829 - val_acc: 0.4754\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2.9000 - acc: 0.4380 - val_loss: 3.0087 - val_acc: 0.4918\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2.8304 - acc: 0.4463 - val_loss: 2.9411 - val_acc: 0.4918\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2.7647 - acc: 0.4463 - val_loss: 2.8759 - val_acc: 0.4754\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2.7100 - acc: 0.4504 - val_loss: 2.8113 - val_acc: 0.4754\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2.6440 - acc: 0.4545 - val_loss: 2.7450 - val_acc: 0.4918\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2.5795 - acc: 0.4545 - val_loss: 2.6858 - val_acc: 0.4754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiEGVnNbO-R6"
      },
      "source": [
        "복잡한 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noD9XuW6SucK"
      },
      "source": [
        "# 표준화\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.fit_transform(X_test)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLVteN5fO_wH",
        "outputId": "529f269b-bb86-43a8-d017-c1b2d4ac013c"
      },
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "\n",
        "# 모델 만들기\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "def model_builder(nodes=16, activation='relu'):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(nodes, activation=activation))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(nodes, activation=activation))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(nodes, activation=activation))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(1, activation='sigmoid')) # 이진분류니까 노드수 1, 활성함수로는 시그모이드\n",
        "\n",
        "  model.compile(optimizer='adam', \n",
        "                loss='binary_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# keras.wrapper를 활용하여 분류기를 만듭니다\n",
        "diff_model = KerasClassifier(build_fn=model_builder, verbose=0)\n",
        "\n",
        "# GridSearch\n",
        "batch_size = [16, 32, 64]\n",
        "epochs = [30, 100, 200]\n",
        "nodes = [32, 64, 128]\n",
        "activation = ['relu', 'sigmoid', 'tanh']\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs, nodes=nodes, activation=activation)\n",
        "\n",
        "\n",
        "# GridSearch CV를 만들기\n",
        "grid = GridSearchCV(estimator=diff_model, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train_scaled, y_train)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  7.9min\n",
            "[Parallel(n_jobs=-1)]: Done 243 out of 243 | elapsed:  9.5min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naa5i9WKQV12",
        "outputId": "08b9b6ea-5a15-4ecf-bbaf-bbbb788c7c11"
      },
      "source": [
        "# 최적의 결과값을 낸 파라미터를 출력합니다\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8429526686668396 using {'activation': 'tanh', 'batch_size': 64, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.8015946547190348, Stdev: 0.027099711170801732 with: {'activation': 'relu', 'batch_size': 16, 'epochs': 30, 'nodes': 32}\n",
            "Means: 0.8015946547190348, Stdev: 0.011168344157138611 with: {'activation': 'relu', 'batch_size': 16, 'epochs': 30, 'nodes': 64}\n",
            "Means: 0.7933642069498698, Stdev: 0.015678575657098605 with: {'activation': 'relu', 'batch_size': 16, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.7686214049657186, Stdev: 0.011043255450719146 with: {'activation': 'relu', 'batch_size': 16, 'epochs': 100, 'nodes': 32}\n",
            "Means: 0.785236636797587, Stdev: 0.02450038569954686 with: {'activation': 'relu', 'batch_size': 16, 'epochs': 100, 'nodes': 64}\n",
            "Means: 0.7684670885403951, Stdev: 0.02661021101102286 with: {'activation': 'relu', 'batch_size': 16, 'epochs': 100, 'nodes': 128}\n",
            "Means: 0.7562242746353149, Stdev: 0.02064303499115904 with: {'activation': 'relu', 'batch_size': 16, 'epochs': 200, 'nodes': 32}\n",
            "Means: 0.7686213850975037, Stdev: 0.02065879512439131 with: {'activation': 'relu', 'batch_size': 16, 'epochs': 200, 'nodes': 64}\n",
            "Means: 0.7934156457583109, Stdev: 0.011103483444538705 with: {'activation': 'relu', 'batch_size': 16, 'epochs': 200, 'nodes': 128}\n",
            "Means: 0.8182098666826884, Stdev: 0.02072424901352349 with: {'activation': 'relu', 'batch_size': 32, 'epochs': 30, 'nodes': 32}\n",
            "Means: 0.8016975323359171, Stdev: 0.009151461918575382 with: {'activation': 'relu', 'batch_size': 32, 'epochs': 30, 'nodes': 64}\n",
            "Means: 0.7851851979891459, Stdev: 0.022719616607733483 with: {'activation': 'relu', 'batch_size': 32, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.76445472240448, Stdev: 0.020207748443941298 with: {'activation': 'relu', 'batch_size': 32, 'epochs': 100, 'nodes': 32}\n",
            "Means: 0.7851337591807047, Stdev: 0.015212594029176291 with: {'activation': 'relu', 'batch_size': 32, 'epochs': 100, 'nodes': 64}\n",
            "Means: 0.7975823084513346, Stdev: 0.01459037923742874 with: {'activation': 'relu', 'batch_size': 32, 'epochs': 100, 'nodes': 128}\n",
            "Means: 0.7685699661572775, Stdev: 0.015717688437177142 with: {'activation': 'relu', 'batch_size': 32, 'epochs': 200, 'nodes': 32}\n",
            "Means: 0.8059670925140381, Stdev: 0.03461319016695033 with: {'activation': 'relu', 'batch_size': 32, 'epochs': 200, 'nodes': 64}\n",
            "Means: 0.785236636797587, Stdev: 0.019926065541990452 with: {'activation': 'relu', 'batch_size': 32, 'epochs': 200, 'nodes': 128}\n",
            "Means: 0.8306584556897482, Stdev: 0.04737785403254391 with: {'activation': 'relu', 'batch_size': 64, 'epochs': 30, 'nodes': 32}\n",
            "Means: 0.7975823084513346, Stdev: 0.01459037923742874 with: {'activation': 'relu', 'batch_size': 64, 'epochs': 30, 'nodes': 64}\n",
            "Means: 0.7645576198895773, Stdev: 0.02587700820394348 with: {'activation': 'relu', 'batch_size': 64, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.8057098786036173, Stdev: 0.01635266984145455 with: {'activation': 'relu', 'batch_size': 64, 'epochs': 100, 'nodes': 32}\n",
            "Means: 0.8098251024881998, Stdev: 0.01657127544014143 with: {'activation': 'relu', 'batch_size': 64, 'epochs': 100, 'nodes': 64}\n",
            "Means: 0.7933642069498698, Stdev: 0.015678575657098605 with: {'activation': 'relu', 'batch_size': 64, 'epochs': 100, 'nodes': 128}\n",
            "Means: 0.7851337591807047, Stdev: 0.005310574889187496 with: {'activation': 'relu', 'batch_size': 64, 'epochs': 200, 'nodes': 32}\n",
            "Means: 0.7810185352961222, Stdev: 0.02067477859059134 with: {'activation': 'relu', 'batch_size': 64, 'epochs': 200, 'nodes': 64}\n",
            "Means: 0.7851337591807047, Stdev: 0.015212594029176291 with: {'activation': 'relu', 'batch_size': 64, 'epochs': 200, 'nodes': 128}\n",
            "Means: 0.8099794189135233, Stdev: 0.022787065890856235 with: {'activation': 'sigmoid', 'batch_size': 16, 'epochs': 30, 'nodes': 32}\n",
            "Means: 0.8347736597061157, Stdev: 0.014748948712316367 with: {'activation': 'sigmoid', 'batch_size': 16, 'epochs': 30, 'nodes': 64}\n",
            "Means: 0.8016975323359171, Stdev: 0.026332711398101775 with: {'activation': 'sigmoid', 'batch_size': 16, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.818107008934021, Stdev: 0.023882509802116862 with: {'activation': 'sigmoid', 'batch_size': 16, 'epochs': 100, 'nodes': 32}\n",
            "Means: 0.8058127562204996, Stdev: 0.004728595423932238 with: {'activation': 'sigmoid', 'batch_size': 16, 'epochs': 100, 'nodes': 64}\n",
            "Means: 0.7933642069498698, Stdev: 0.015678575657098605 with: {'activation': 'sigmoid', 'batch_size': 16, 'epochs': 100, 'nodes': 128}\n",
            "Means: 0.7893518606821696, Stdev: 0.01636820628348882 with: {'activation': 'sigmoid', 'batch_size': 16, 'epochs': 200, 'nodes': 32}\n",
            "Means: 0.7933642069498698, Stdev: 0.006526631841348462 with: {'activation': 'sigmoid', 'batch_size': 16, 'epochs': 200, 'nodes': 64}\n",
            "Means: 0.7933127482732137, Stdev: 0.016410034033806965 with: {'activation': 'sigmoid', 'batch_size': 16, 'epochs': 200, 'nodes': 128}\n",
            "Means: 0.6972736716270447, Stdev: 0.1861603485130763 with: {'activation': 'sigmoid', 'batch_size': 32, 'epochs': 30, 'nodes': 32}\n",
            "Means: 0.6487654248873392, Stdev: 0.06552713093730803 with: {'activation': 'sigmoid', 'batch_size': 32, 'epochs': 30, 'nodes': 64}\n",
            "Means: 0.6947016417980194, Stdev: 0.18601528938717293 with: {'activation': 'sigmoid', 'batch_size': 32, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.814146081606547, Stdev: 0.026060387914404366 with: {'activation': 'sigmoid', 'batch_size': 32, 'epochs': 100, 'nodes': 32}\n",
            "Means: 0.822376529375712, Stdev: 0.022821063607470597 with: {'activation': 'sigmoid', 'batch_size': 32, 'epochs': 100, 'nodes': 64}\n",
            "Means: 0.8057613174120585, Stdev: 0.025527658651744806 with: {'activation': 'sigmoid', 'batch_size': 32, 'epochs': 100, 'nodes': 128}\n",
            "Means: 0.8182098666826884, Stdev: 0.00480134088446204 with: {'activation': 'sigmoid', 'batch_size': 32, 'epochs': 200, 'nodes': 32}\n",
            "Means: 0.7975308696428934, Stdev: 0.015220766888294759 with: {'activation': 'sigmoid', 'batch_size': 32, 'epochs': 200, 'nodes': 64}\n",
            "Means: 0.7891460855801901, Stdev: 0.021368613940567136 with: {'activation': 'sigmoid', 'batch_size': 32, 'epochs': 200, 'nodes': 128}\n",
            "Means: 0.6154835422833761, Stdev: 0.11704060473891872 with: {'activation': 'sigmoid', 'batch_size': 64, 'epochs': 30, 'nodes': 32}\n",
            "Means: 0.603600819905599, Stdev: 0.07878388070938477 with: {'activation': 'sigmoid', 'batch_size': 64, 'epochs': 30, 'nodes': 64}\n",
            "Means: 0.5249485671520233, Stdev: 0.06605326695627649 with: {'activation': 'sigmoid', 'batch_size': 64, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.8182098865509033, Stdev: 0.050629194495689436 with: {'activation': 'sigmoid', 'batch_size': 64, 'epochs': 100, 'nodes': 32}\n",
            "Means: 0.809876541296641, Stdev: 0.021335757290060608 with: {'activation': 'sigmoid', 'batch_size': 64, 'epochs': 100, 'nodes': 64}\n",
            "Means: 0.8182098666826884, Stdev: 0.00480134088446204 with: {'activation': 'sigmoid', 'batch_size': 64, 'epochs': 100, 'nodes': 128}\n",
            "Means: 0.8140432039896647, Stdev: 0.010139089949215817 with: {'activation': 'sigmoid', 'batch_size': 64, 'epochs': 200, 'nodes': 32}\n",
            "Means: 0.7850308616956075, Stdev: 0.016709959137967006 with: {'activation': 'sigmoid', 'batch_size': 64, 'epochs': 200, 'nodes': 64}\n",
            "Means: 0.7893004218737284, Stdev: 0.0169075139054296 with: {'activation': 'sigmoid', 'batch_size': 64, 'epochs': 200, 'nodes': 128}\n",
            "Means: 0.8099279801050822, Stdev: 0.0053581941547767905 with: {'activation': 'tanh', 'batch_size': 16, 'epochs': 30, 'nodes': 32}\n",
            "Means: 0.8182098666826884, Stdev: 0.01116527115704036 with: {'activation': 'tanh', 'batch_size': 16, 'epochs': 30, 'nodes': 64}\n",
            "Means: 0.8182098666826884, Stdev: 0.01116527115704036 with: {'activation': 'tanh', 'batch_size': 16, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.8058127562204996, Stdev: 0.011134182520718058 with: {'activation': 'tanh', 'batch_size': 16, 'epochs': 100, 'nodes': 32}\n",
            "Means: 0.8139917651812235, Stdev: 0.01806953216111865 with: {'activation': 'tanh', 'batch_size': 16, 'epochs': 100, 'nodes': 64}\n",
            "Means: 0.7852880557378134, Stdev: 0.03456821356637427 with: {'activation': 'tanh', 'batch_size': 16, 'epochs': 100, 'nodes': 128}\n",
            "Means: 0.7849794228871664, Stdev: 0.026709369488646467 with: {'activation': 'tanh', 'batch_size': 16, 'epochs': 200, 'nodes': 32}\n",
            "Means: 0.7727366288503011, Stdev: 0.015204761865651645 with: {'activation': 'tanh', 'batch_size': 16, 'epochs': 200, 'nodes': 64}\n",
            "Means: 0.7894032796223959, Stdev: 0.03561781981911005 with: {'activation': 'tanh', 'batch_size': 16, 'epochs': 200, 'nodes': 128}\n",
            "Means: 0.826337456703186, Stdev: 0.027445821227672643 with: {'activation': 'tanh', 'batch_size': 32, 'epochs': 30, 'nodes': 32}\n",
            "Means: 0.818107008934021, Stdev: 0.023882509802116862 with: {'activation': 'tanh', 'batch_size': 32, 'epochs': 30, 'nodes': 64}\n",
            "Means: 0.8058641950289408, Stdev: 0.037786462627722 with: {'activation': 'tanh', 'batch_size': 32, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.8182613054911295, Stdev: 0.014507440365110652 with: {'activation': 'tanh', 'batch_size': 32, 'epochs': 100, 'nodes': 32}\n",
            "Means: 0.8140432039896647, Stdev: 0.010139089949215817 with: {'activation': 'tanh', 'batch_size': 32, 'epochs': 100, 'nodes': 64}\n",
            "Means: 0.7976337472597758, Stdev: 0.019996453928639156 with: {'activation': 'tanh', 'batch_size': 32, 'epochs': 100, 'nodes': 128}\n",
            "Means: 0.7976851860682169, Stdev: 0.029903497465578018 with: {'activation': 'tanh', 'batch_size': 32, 'epochs': 200, 'nodes': 32}\n",
            "Means: 0.7893004218737284, Stdev: 0.0169075139054296 with: {'activation': 'tanh', 'batch_size': 32, 'epochs': 200, 'nodes': 64}\n",
            "Means: 0.7933642069498698, Stdev: 0.006526631841348462 with: {'activation': 'tanh', 'batch_size': 32, 'epochs': 200, 'nodes': 128}\n",
            "Means: 0.8222736517588297, Stdev: 0.01222180592518424 with: {'activation': 'tanh', 'batch_size': 64, 'epochs': 30, 'nodes': 32}\n",
            "Means: 0.8056069811185201, Stdev: 0.03210477893044874 with: {'activation': 'tanh', 'batch_size': 64, 'epochs': 30, 'nodes': 64}\n",
            "Means: 0.8429526686668396, Stdev: 0.03549077506748092 with: {'activation': 'tanh', 'batch_size': 64, 'epochs': 30, 'nodes': 128}\n",
            "Means: 0.8017489711443583, Stdev: 0.01928510565100901 with: {'activation': 'tanh', 'batch_size': 64, 'epochs': 100, 'nodes': 32}\n",
            "Means: 0.8058641950289408, Stdev: 0.014439248223472837 with: {'activation': 'tanh', 'batch_size': 64, 'epochs': 100, 'nodes': 64}\n",
            "Means: 0.7810699741045634, Stdev: 0.01430299854208455 with: {'activation': 'tanh', 'batch_size': 64, 'epochs': 100, 'nodes': 128}\n",
            "Means: 0.8139917651812235, Stdev: 0.01806953216111865 with: {'activation': 'tanh', 'batch_size': 64, 'epochs': 200, 'nodes': 32}\n",
            "Means: 0.7935185035069784, Stdev: 0.03755046016553819 with: {'activation': 'tanh', 'batch_size': 64, 'epochs': 200, 'nodes': 64}\n",
            "Means: 0.8016975323359171, Stdev: 0.009151461918575382 with: {'activation': 'tanh', 'batch_size': 64, 'epochs': 200, 'nodes': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSzxsysRQrgg",
        "outputId": "94478641-2f3c-4169-a04a-518bb9d49cf3"
      },
      "source": [
        "def model_builder(nodes=16, activation='relu'):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(nodes, activation=activation))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(nodes, activation=activation))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(nodes, activation=activation))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(1, activation='sigmoid')) # 이진분류니까 노드수 1, 활성함수로는 시그모이드\n",
        "\n",
        "  model.compile(optimizer='adam', \n",
        "                loss='binary_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        " \n",
        "results = model_builder(nodes = 128, activation='tanh').fit(X_train_scaled, y_train, batch_size=64, epochs=30, validation_data=(X_test_scaled,y_test))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "4/4 [==============================] - 1s 69ms/step - loss: 0.6457 - accuracy: 0.6818 - val_loss: 0.4805 - val_accuracy: 0.8197\n",
            "Epoch 2/30\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3868 - accuracy: 0.8264 - val_loss: 0.4483 - val_accuracy: 0.7869\n",
            "Epoch 3/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3453 - accuracy: 0.8719 - val_loss: 0.4508 - val_accuracy: 0.7869\n",
            "Epoch 4/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3344 - accuracy: 0.8719 - val_loss: 0.4616 - val_accuracy: 0.7705\n",
            "Epoch 5/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3345 - accuracy: 0.8595 - val_loss: 0.4701 - val_accuracy: 0.7705\n",
            "Epoch 6/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2934 - accuracy: 0.8843 - val_loss: 0.4675 - val_accuracy: 0.7869\n",
            "Epoch 7/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2842 - accuracy: 0.8760 - val_loss: 0.4676 - val_accuracy: 0.7705\n",
            "Epoch 8/30\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.2865 - accuracy: 0.8802 - val_loss: 0.4647 - val_accuracy: 0.8033\n",
            "Epoch 9/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2638 - accuracy: 0.8802 - val_loss: 0.4518 - val_accuracy: 0.8033\n",
            "Epoch 10/30\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2182 - accuracy: 0.9298 - val_loss: 0.4455 - val_accuracy: 0.7869\n",
            "Epoch 11/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2370 - accuracy: 0.9174 - val_loss: 0.4381 - val_accuracy: 0.7869\n",
            "Epoch 12/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2147 - accuracy: 0.9091 - val_loss: 0.4315 - val_accuracy: 0.7869\n",
            "Epoch 13/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1997 - accuracy: 0.9380 - val_loss: 0.4317 - val_accuracy: 0.8033\n",
            "Epoch 14/30\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1888 - accuracy: 0.9339 - val_loss: 0.4366 - val_accuracy: 0.7869\n",
            "Epoch 15/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2015 - accuracy: 0.9174 - val_loss: 0.4461 - val_accuracy: 0.7869\n",
            "Epoch 16/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1970 - accuracy: 0.9380 - val_loss: 0.4463 - val_accuracy: 0.8197\n",
            "Epoch 17/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2233 - accuracy: 0.9008 - val_loss: 0.4437 - val_accuracy: 0.7869\n",
            "Epoch 18/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1632 - accuracy: 0.9339 - val_loss: 0.4428 - val_accuracy: 0.8197\n",
            "Epoch 19/30\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1754 - accuracy: 0.9421 - val_loss: 0.4400 - val_accuracy: 0.8361\n",
            "Epoch 20/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1595 - accuracy: 0.9380 - val_loss: 0.4434 - val_accuracy: 0.8033\n",
            "Epoch 21/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1461 - accuracy: 0.9545 - val_loss: 0.4378 - val_accuracy: 0.8033\n",
            "Epoch 22/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1291 - accuracy: 0.9669 - val_loss: 0.4324 - val_accuracy: 0.7869\n",
            "Epoch 23/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1527 - accuracy: 0.9504 - val_loss: 0.4273 - val_accuracy: 0.8033\n",
            "Epoch 24/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1439 - accuracy: 0.9504 - val_loss: 0.4436 - val_accuracy: 0.8033\n",
            "Epoch 25/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1293 - accuracy: 0.9587 - val_loss: 0.4526 - val_accuracy: 0.8033\n",
            "Epoch 26/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1451 - accuracy: 0.9298 - val_loss: 0.4632 - val_accuracy: 0.7869\n",
            "Epoch 27/30\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1521 - accuracy: 0.9421 - val_loss: 0.4498 - val_accuracy: 0.8033\n",
            "Epoch 28/30\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1361 - accuracy: 0.9669 - val_loss: 0.4627 - val_accuracy: 0.7869\n",
            "Epoch 29/30\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.1421 - accuracy: 0.9628 - val_loss: 0.4838 - val_accuracy: 0.7705\n",
            "Epoch 30/30\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1217 - accuracy: 0.9545 - val_loss: 0.4995 - val_accuracy: 0.7869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJP_Nh-iTHl3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}