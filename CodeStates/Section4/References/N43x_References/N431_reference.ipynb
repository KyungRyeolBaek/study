{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10OXb-oNKVzO"
   },
   "source": [
    "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
    "\n",
    "## *DATA SCIENCE / SECTION 4 / SPRINT 3 / Assignment 1*\n",
    "# Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model 을 사용해보기\n",
    "- <a href=\"#p2\">Part 2:</a> Custom CNN Model을 제작해보기\n",
    "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
    "\n",
    "\n",
    "케라스를 이용한 바이너리 이미지 분류 모델에 3가지 CNN 모델을 적용한다.  <br>\n",
    "[데이터 다운로드](https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/mountainForest.zip) <br>\n",
    "산의 이미지(./data/mountin/*)와 숲의 이미지(./data/forest/*)를 분류하십시오. 산을 포스티브 클래스(1)로, 숲 이미지를 네거티브(0)로 처리한다.\n",
    "\n",
    "|Mountain (+)|Forest (-)|\n",
    "|---|---|\n",
    "|![](https://github.com/codestates/ds-cs-section4-sprint3/blob/main/N431/data/mountain/art1131.jpg?raw=1)|![](https://github.com/codestates/ds-cs-section4-sprint3/blob/main/N431/data/forest/cdmc317.jpg?raw=1)|\n",
    "\n",
    "표본이 작다는 점을 감안하면 문제는 현실적으로 어렵다. 즉, 클래스당 약 350개의 관측치가 있다. 이 샘플 크기는 직장에서 이미지 분류 문제/솔루션을 프로토타이핑할 때 기대할 수 있는 것일 수 있다. 이럴 때에는 여러가지 모델을 적용해보는 것이 중요하기 때문에 이번 sprint에서는 가능한 여러 모델을 평가하는 데 익숙해지는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-Ng0p_35yrh"
   },
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI3qR2nVKVzT"
   },
   "source": [
    "## Pre - Trained Model\n",
    "<a id=\"p1\"></a>\n",
    "\n",
    "Keras에서 제공하는 pre-trained 모델인 ResNet50을 불러오고, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - 이 모델은 50 개의 layer를 가진  CNN기반의 모델입니다. 총 [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt)를 분류하는 모델로 아래와 같이 사용할 수 있습니다. 우리가 풀어야 할 과제는 2가지 이니, 마지막 출력 단을 변경해서 사용하면 됩니다.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D()\n",
    "from tensorflow.keras.models import Model # This is the functional API\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "```\n",
    "\n",
    "`ResNet50`을 불러올 때, `include_top`을 False로 하면, 기존 1000개의 분류 문제를 풀 수 있는 ResNet 모델에서 Fully Connected layer를 제거해주는 역할을 합니다. \n",
    "\n",
    "```python\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "```\n",
    "이 부분은 ResNet50 레이어들의 파라미터 학습모드를 끄는(off) 것입니다. 이렇게 설정된 매개 변수는 설령 지금 순전파/역전파 이후 오류가 전파 되더라도 파라미터가 학습(업데이트)되지 않는 것입니다. \n",
    "\n",
    "Keras 기능 API를 사용하여 모델에 추가로 `Fully-conneted layer`(Dense모델)을 추가해야합니다. 우리는 최상위 레이어,  `Fully-conneted layer`를 제거합니다. 출력층의 숫자(=분류할 클래스의 개수)가 맞지 않기 때문이죠. 네트워크의 특징을 추출하는 부분 만 유지하는 것입니다. `GlobalAveragePooling2D` 레이어는 마지막 컨벌루션 레이어 출력 (2 차원) 각각의 평균을 취함으로써 정말 멋진 평탄화 기능으로 작동합니다.\n",
    "\n",
    "```python\n",
    "x = res.output\n",
    "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x) # 출력층 설계\n",
    "model = Model(res.input, predictions)\n",
    "```\n",
    "\n",
    "이 과제는 산의 이미지 (`. / data / mountain / *`)와 숲의 이미지 (`. / data / forest / *`)를 분류하기 위해 위의 전이 학습(transfer learning)을 적용하는 것입니다. 산을 postive 클래스 (1)로, 숲 이미지를 음수 (0)로 취급합니다.\n",
    "\n",
    "Steps to complete assignment: \n",
    "1. 이미지 데이터를 numpy 배열 (`X`)로 불러옵니다.\n",
    "2. 레이블에 대한 `y`를 만듭니다.\n",
    "3. resnet의 사전 훈련 된(pre-trained) 레이어로 모델 훈련\n",
    "4. 모델의 정확성보고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jawDD7ET2es-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    " \n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model # This is the functional API\n",
    " \n",
    "resnet = ResNet50(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-aocBlc9VAN",
    "outputId": "a2f6b168-0933-4457-dde4-3e85463fc637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vqFA8Q0T8tS-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/content/drive/My Drive/N431'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "train_forest_dir = os.path.join(train_dir, 'forest')\n",
    "train_mountain_dir = os.path.join(train_dir, 'mountain')\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "validation_forest_dir = os.path.join(validation_dir, 'forest')\n",
    "validation_mountain_dir = os.path.join(validation_dir, 'mountain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QcoQaYVJgbB",
    "outputId": "71c6c094-f1f1-4125-a1d3-96ee4511db9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', '.ipynb_checkpoints', 'art114.jpg', 'for102.jpg', 'for105.jpg', 'for106.jpg', 'for110.jpg', 'for112.jpg', 'for114.jpg', 'for116.jpg']\n",
      "['.DS_Store', '.ipynb_checkpoints', 'art1131.jpg', 'art1132.jpg', 'gre242.jpg', 'land10.jpg', 'land11.jpg', 'land13.jpg', 'land130.jpg', 'land131.jpg']\n"
     ]
    }
   ],
   "source": [
    "train_forest_fnames = os.listdir(train_forest_dir)\n",
    "train_forest_fnames.sort()\n",
    "print(train_forest_fnames[:10])\n",
    "\n",
    "train_mountain_fnames = os.listdir(train_mountain_dir)\n",
    "train_mountain_fnames.sort()\n",
    "print(train_mountain_fnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIHdwy64KAnb"
   },
   "outputs": [],
   "source": [
    "print('total training forest images:', len(os.listdir(train_forest_dir)))\n",
    "print('total training mountain images:', len(os.listdir(train_mountain_dir)))\n",
    "print('total validation forest images:', len(os.listdir(validation_forest_dir)))\n",
    "print('total validation mountain images:', len(os.listdir(validation_mountain_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0t-KIeWgKsjY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "O_G8ZO-hGOyc"
   },
   "outputs": [],
   "source": [
    "img_input = layers.Input(shape=(224*224*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdZPnG3iLUW4",
    "outputId": "8655f591-9b32-44f1-dc2f-4b30359fd031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 533 images belonging to 2 classes.\n",
      "Found 195 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  \n",
    "        target_size=(224, 224),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4sT1_tMb2jHp"
   },
   "outputs": [],
   "source": [
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5O5qokCY20RU"
   },
   "outputs": [],
   "source": [
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(resnet.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "m0WLIhWJ3WVL"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhoBx_dJOxI9",
    "outputId": "347b7dea-ff64-41b9-cc0d-a90cef90786e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "27/27 [==============================] - 223s 7s/step - loss: 1.0484 - accuracy: 0.4713 - val_loss: 0.5902 - val_accuracy: 0.6667\n",
      "Epoch 2/15\n",
      "27/27 [==============================] - 3s 116ms/step - loss: 0.7410 - accuracy: 0.5315 - val_loss: 0.5521 - val_accuracy: 0.7333\n",
      "Epoch 3/15\n",
      "27/27 [==============================] - 3s 116ms/step - loss: 0.5540 - accuracy: 0.7387 - val_loss: 0.5056 - val_accuracy: 0.7795\n",
      "Epoch 4/15\n",
      "27/27 [==============================] - 3s 115ms/step - loss: 0.5163 - accuracy: 0.7720 - val_loss: 0.5259 - val_accuracy: 0.8154\n",
      "Epoch 5/15\n",
      "27/27 [==============================] - 3s 115ms/step - loss: 0.4618 - accuracy: 0.8660 - val_loss: 0.4310 - val_accuracy: 0.7949\n",
      "Epoch 6/15\n",
      "27/27 [==============================] - 3s 114ms/step - loss: 0.4219 - accuracy: 0.8540 - val_loss: 0.5192 - val_accuracy: 0.7795\n",
      "Epoch 7/15\n",
      "27/27 [==============================] - 3s 117ms/step - loss: 0.3792 - accuracy: 0.8714 - val_loss: 0.4927 - val_accuracy: 0.8000\n",
      "Epoch 8/15\n",
      "27/27 [==============================] - 3s 124ms/step - loss: 0.3644 - accuracy: 0.8747 - val_loss: 0.3548 - val_accuracy: 0.8564\n",
      "Epoch 9/15\n",
      "27/27 [==============================] - 3s 125ms/step - loss: 0.3471 - accuracy: 0.8901 - val_loss: 0.4854 - val_accuracy: 0.8000\n",
      "Epoch 10/15\n",
      "27/27 [==============================] - 3s 124ms/step - loss: 0.3932 - accuracy: 0.8230 - val_loss: 0.3255 - val_accuracy: 0.8974\n",
      "Epoch 11/15\n",
      "27/27 [==============================] - 3s 124ms/step - loss: 0.2541 - accuracy: 0.9336 - val_loss: 0.3291 - val_accuracy: 0.8872\n",
      "Epoch 12/15\n",
      "27/27 [==============================] - 3s 116ms/step - loss: 0.2615 - accuracy: 0.9154 - val_loss: 0.2974 - val_accuracy: 0.9026\n",
      "Epoch 13/15\n",
      "27/27 [==============================] - 3s 117ms/step - loss: 0.2693 - accuracy: 0.8958 - val_loss: 0.4212 - val_accuracy: 0.7692\n",
      "Epoch 14/15\n",
      "27/27 [==============================] - 3s 114ms/step - loss: 0.4230 - accuracy: 0.7764 - val_loss: 0.2809 - val_accuracy: 0.9077\n",
      "Epoch 15/15\n",
      "27/27 [==============================] - 3s 115ms/step - loss: 0.2479 - accuracy: 0.9284 - val_loss: 0.3302 - val_accuracy: 0.8103\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=27,  \n",
    "      epochs=15,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=10,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9J2pnATN4L8",
    "outputId": "163aad63-b806-4cba-dcd8-c14195e3ed7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 225ms/step - loss: 26.0850 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[26.084976196289062, 0.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjBDRtCw55Sb"
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gpEjsFpKVzj"
   },
   "source": [
    "## Custom CNN Model\n",
    "\n",
    "\n",
    "이 단계에서는 Keras를 사용하여 자신 만의 CNN을 작성하고 훈련합니다. 네트워크 시작 부분에 적어도 하나의 Conv 레이어와 pooling 레이어가있는 아키텍처를 사용할 수 있습니다. 원하는 경우 더 추가 해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VuRqx-vXO9k0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ogD_7uvF5sVw"
   },
   "outputs": [],
   "source": [
    "model_scratch = Sequential()\n",
    "model_scratch.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model_scratch.add(MaxPooling2D(2,2))\n",
    "model_scratch.add(Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model_scratch.add(MaxPooling2D(2,2))\n",
    "model_scratch.add(Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model_scratch.add(Flatten())\n",
    "model_scratch.add(Dense(128, activation='relu'))\n",
    "model_scratch.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sxnCSKh34uLv"
   },
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model_scratch.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vgwLFkhUKVzj"
   },
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model_scratch.fit(X_train, y_train, batch_size=32, epochs=15, validation_data=(X_validation, y_validation))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "N431_reference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
